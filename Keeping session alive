import org.apache.spark.sql.functions._
import org.apache.spark.sql.SparkSession

val spark = SparkSession.builder()
  .appName("example")
  .master("local[*]")  // Adjust this based on your setup
  .getOrCreate()

import spark.implicits._

// Assuming your original DataFrame doesn't have a value column
val df = Seq(
  ("A"),
  ("A"),
  ("B"),
  ("B"),
  ("B"),
  ("C"),
  ("C"),
  ("D"),
  ("D")
).toDF("key")

// Generating a unique identifier using monotonically_increasing_id()
val dfWithId = df.withColumn("value", monotonically_increasing_id())

// Optionally, cache the DataFrame if it's going to be reused
dfWithId.cache()

// Calculate the total count
val totalCount = dfWithId.count()

// Calculate the range dynamically
val range = totalCount match {
  case count if count < 50 => 2
  case count => (count / 10).toInt
}

// Add a partition ID to each row based on the calculated range
val dfWithPartitionId = dfWithId.withColumn("partition_id", floor(col("value") / range))

// Group by the partition ID and key, then aggregate
val aggregatedDF = dfWithPartitionId.groupBy("partition_id", "key")
  .agg(collect_list("value").alias("values"))

aggregatedDF.show()
