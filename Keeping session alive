**Preferred worker type** is the VM family/size Databricks uses for your Spark worker nodes; it directly controls cores, RAM, storage per worker, and therefore cluster performance and cloud+DBU cost. [flexera](https://www.flexera.com/blog/finops/databricks-clusters/)

## What “preferred worker type” means
- It corresponds 1:1 to a **cloud VM instance type** (e.g., `r6i.2xlarge`, `i3.xlarge`, your `rd-fleet.xlarge`). [linkedin](https://www.linkedin.com/pulse/choosing-right-databricks-cluster-comprehensive-guide-teixeira-6tf8f)
- Each worker of that type runs Spark executors; your cluster capacity is roughly: *workers × (cores, RAM of that VM)*. [flexera](https://www.flexera.com/blog/finops/databricks-clusters/)
- With autoscaling, Databricks adds/removes workers of this type between Min and Max values. [docs.databricks](https://docs.databricks.com/aws/en/compute/configure)

## How it affects compute and DBU
- **Performance**:  
  - More cores ⇒ more task parallelism.  
  - More RAM ⇒ can cache larger datasets, less spilling/shuffle issues. [community.databricks](https://community.databricks.com/t5/data-engineering/how-do-i-know-which-worker-type-to-choose-when-creating-my/td-p/24375)
  - Local SSD/HDD on storage‑optimized nodes (like i3) ⇒ faster shuffles and joins. [flexera](https://www.flexera.com/blog/finops/databricks-clusters/)
- **Cloud cost**: hourly VM price scales with instance size/family; large or SSD‑heavy nodes cost more per hour. [cloudchipr](https://cloudchipr.com/blog/databricks-pricing)
- **DBU cost**: DBUs are charged per node and scale with its compute power; smaller instance type → fewer DBUs/hour, larger or GPU type → more DBUs/hour. [finout](https://www.finout.io/blog/databricks-pricing)
- Changing worker type is like moving from, say, 16‑core, 64‑GB physical worker to 8‑core, 32‑GB worker in your on‑prem YARN cluster: fewer resources per container and lower power, but cheaper.

## When to use which types
Typical mapping to workload patterns: [youtube](https://www.youtube.com/watch?v=n44rbwGGS-M)

- **General purpose (m5d / D-series / similar)**: Mixed ETL + SQL, dev/test; balanced CPU/RAM, good default.  
- **Compute‑optimized (c5 / F-series)**: CPU‑heavy, light data per task, iterative ML, streaming with lots of small tasks.  
- **Memory‑optimized (r5 / E-series)**: Wide aggregations, joins, window functions, lots of caching; think “big DIM+FACT joins”.  
- **Storage‑optimized (i3 / Ls-series)**: Heavy shuffles, multi‑way joins, skewed data where local SSD for spill/shuffle helps; what you see as “storage optimized (Delta cache accelerated)”. [youtube](https://www.youtube.com/watch?v=n44rbwGGS-M)
- **GPU**: Deep learning / heavy inference.

You choose “preferred worker type” based on where your current jobs are bottlenecked (CPU vs memory vs I/O); you can confirm using Ganglia/Spark UI metrics, just like checking YARN node utilization. [community.databricks](https://community.databricks.com/t5/data-engineering/how-do-i-know-which-worker-type-to-choose-when-creating-my/td-p/24375)

## When not to change it
- For small dev clusters where performance is fine, changing to bigger workers just burns DBUs without benefit. [cloudoptimo](https://www.cloudoptimo.com/blog/a-complete-guide-to-databricks-pricing-and-cost-management/)
- If your jobs are clearly **CPU‑bound**, switching to memory‑optimized without adding cores may not help.  
- If your data volume is tiny, moving to storage‑optimized SSD nodes won’t pay off.

## Relating to a physical/YARN cluster
In your on‑prem YARN world:  

- **Worker type ≈ node hardware profile**.  
  - Example: you might have a “standard” node (16 vcores, 64 GB) and a “memory‑heavy” node (16 vcores, 256 GB); queues can be mapped to one or the other.  
- **Cluster size** is *number of nodes in the YARN queue*; in Databricks it is *Min/Max workers × resources per worker type*. [docs.databricks](https://docs.databricks.com/aws/en/compute/configure)
- **Instance pool** in Databricks ≈ slice of that hardware pool reserved for a team; all workers pulled from that pool still use the worker type defined there. [docs.databricks](https://docs.databricks.com/aws/en/compute/pool-best-practices)

So: **preferred worker type** defines the *shape and cost per node*; **Min/Max workers + pools/policies** define *how many of those nodes* your team can consume, similar to how many physical nodes and which hardware class your YARN queue can use.
