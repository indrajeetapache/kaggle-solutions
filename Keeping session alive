val rdd = spark.sparkContext.textFile(filePath)

// Accumulate lines in each partition and concatenate to a single string
val rawStr = rdd.mapPartitions { iter =>
  Iterator(iter.mkString("\n"))
}.collect().mkString("\n")



=======


import org.apache.hadoop.fs.{FileSystem, Path}
import scala.io.Source
import org.apache.spark.SparkContext

val fs = FileSystem.get(spark.sparkContext.hadoopConfiguration)
val path = new Path(filePath)

// Read the file using BufferedSource
val rawStr = {
  val stream = fs.open(path)
  try {
    Source.fromInputStream(stream).mkString
  } finally {
    stream.close()
  }
}


====

import cats.effect.{IO, Resource}
import fs2.Stream
import fs2.io.readInputStream
import org.apache.hadoop.conf.Configuration
import org.apache.hadoop.fs.{FileSystem, Path}

// Function to read a file from HDFS line by line and return as a single String
def readHDFSFileLineByLine(hdfsPath: String): IO[String] = {
  val configuration = new Configuration()
  val fs = FileSystem.get(configuration)

  def inputStreamResource: Resource[IO, java.io.InputStream] = Resource.make {
    IO(fs.open(new Path(hdfsPath)))  // Acquire InputStream from HDFS
  } { inputStream =>
    IO(inputStream.close()).handleErrorWith(_ => IO.unit)  // Ensure stream is closed
  }

  inputStreamResource.use { inputStream =>
    readInputStream(IO.pure(inputStream), chunkSize = 8192, closeAfterUse = false)
      .through(fs2.text.utf8.decode)   // Decode byte stream to text
      .through(fs2.text.lines)         // Split the decoded text into lines
      .intersperse("\n")               // Add line separators back (preserving line breaks)
      .compile
      .string                          // Concatenate all lines into a single string
  }
}


<dependencies>
    <!-- FS2 Core and IO modules -->
    <dependency>
        <groupId>co.fs2</groupId>
        <artifactId>fs2-core_2.13</artifactId>
        <version>3.2.7</version> <!-- Use the latest stable version -->
    </dependency>
    <dependency>
        <groupId>co.fs2</groupId>
        <artifactId>fs2-io_2.13</artifactId>
        <version>3.2.7</version>
    </dependency>

    <!-- Cats Effect for IO and Resource Management -->
    <dependency>
        <groupId>org.typelevel</groupId>
        <artifactId>cats-effect_2.13</artifactId>
        <version>3.3.14</version> <!-- Use the latest stable version -->
    </dependency>
</dependencies>

=================

import cats.effect.{IO, Resource}
import fs2.Stream
import fs2.io.readInputStream
import org.apache.hadoop.conf.Configuration
import org.apache.hadoop.fs.{FileSystem, Path}

// Function to read a file from HDFS line by line and return as a single String
def readHDFSFileLineByLine(hdfsPath: String): IO[String] = {
  val configuration = new Configuration()
  val fs = FileSystem.get(configuration)

  // Create an InputStream resource for managing the HDFS InputStream lifecycle
  def inputStreamResource: Resource[IO, java.io.InputStream] = Resource.make {
    IO(fs.open(new Path(hdfsPath)))  // Acquire InputStream from HDFS
  } { inputStream =>
    IO(inputStream.close()).handleErrorWith(_ => IO.unit)  // Ensure the stream is closed on release
  }

  inputStreamResource.use { inputStream =>
    readInputStream(IO.pure(inputStream), chunkSize = 8192) // `closeAfterUse` is not required in fs2 2.x
      .through(fs2.text.utf8Decode)  // Decode byte stream to text
      .through(fs2.text.lines)       // Split the decoded text into lines
      .intersperse("\n")             // Add line separators back to preserve line breaks
      .compile
      .string                        // Concatenate all lines into a single string
  }
}

========

import cats.effect.{ContextShift, IO, Resource}
import fs2.Stream
import fs2.io.readInputStream
import org.apache.hadoop.conf.Configuration
import org.apache.hadoop.fs.{FileSystem, Path}
import scala.concurrent.ExecutionContext

// Define your ContextShift and Blocker
implicit val cs: ContextShift[IO] = IO.contextShift(ExecutionContext.global)

def readHDFSFileLineByLine(hdfsPath: String): IO[String] = {
  val configuration = new Configuration()
  val fs = FileSystem.get(configuration)

  // Resource for HDFS InputStream with logging
  def inputStreamResource: Resource[IO, java.io.InputStream] = Resource.make {
    IO {
      println(s"Opening HDFS file at path: $hdfsPath")
      fs.open(new Path(hdfsPath))
    }
  } { inputStream =>
    IO {
      println(s"Closing HDFS file at path: $hdfsPath")
      inputStream.close()
    }.handleErrorWith { ex =>
      IO(println(s"Error closing HDFS file at path: $hdfsPath: $ex"))
    }
  }

  inputStreamResource.use { inputStream =>
    readInputStream(IO.pure(inputStream), chunkSize = 8192) // Removed `blocker`
      .through(fs2.text.utf8Decode)
      .through(fs2.text.lines)
      .intersperse("\n")
      .compile
      .string
      .flatTap(content => IO(println(s"Finished reading HDFS file at path: $hdfsPath, content size: ${content.length}")))
  }
}
==============


import cats.effect.{ContextShift, IO, Resource, Sync}
import fs2.Stream
import fs2.io.readInputStream
import org.apache.hadoop.conf.Configuration
import org.apache.hadoop.fs.{FileSystem, Path, FSDataInputStream}
import scala.concurrent.ExecutionContext

// Define your ContextShift and Sync instances
implicit val cs: ContextShift[IO] = IO.contextShift(ExecutionContext.global)
implicit val syncIO: Sync[IO] = IO.ioConcurrentEffect

def readHDFSFileLineByLine(hdfsPath: String): IO[String] = {
  val configuration = new Configuration()
  val fs = FileSystem.get(configuration)

  // Resource for HDFS InputStream with logging
  def inputStreamResource: Resource[IO, java.io.InputStream] = Resource.make {
    IO {
      println(s"Opening HDFS file at path: $hdfsPath")
      fs.open(new Path(hdfsPath))  // fs.open returns FSDataInputStream
    }.widen[InputStream]  // Widen to InputStream to match expected type
  } { inputStream =>
    IO {
      println(s"Closing HDFS file at path: $hdfsPath")
      inputStream.close()
    }.handleErrorWith { ex =>
      IO(println(s"Error closing HDFS file at path: $hdfsPath: $ex"))
    }
  }

  inputStreamResource.use { inputStream =>
    readInputStream(IO.pure(inputStream), chunkSize = 8192)
      .through(fs2.text.utf8Decode)
      .through(fs2.text.lines)
      .intersperse("\n")
      .compile
      .string
      .flatTap(content => IO(println(s"Finished reading HDFS file at path: $hdfsPath, content size: ${content.length}")))
  }
}
=============


import cats.effect.{Blocker, ContextShift, IO, Resource, Sync}
import fs2.Stream
import fs2.io.readInputStream
import org.apache.hadoop.conf.Configuration
import org.apache.hadoop.fs.{FileSystem, Path, FSDataInputStream}
import scala.concurrent.ExecutionContext

// Define your ContextShift and Blocker
implicit val cs: ContextShift[IO] = IO.contextShift(ExecutionContext.global)
val blocker: Blocker = Blocker.liftExecutionContext(ExecutionContext.global)

def readHDFSFileLineByLine(hdfsPath: String): IO[String] = {
  val configuration = new Configuration()
  val fs = FileSystem.get(configuration)

  // Resource for HDFS InputStream with logging
  def inputStreamResource: Resource[IO, java.io.InputStream] = Resource.make {
    IO {
      println(s"Opening HDFS file at path: $hdfsPath")
      fs.open(new Path(hdfsPath))
    }
  } { inputStream =>
    IO {
      println(s"Closing HDFS file at path: $hdfsPath")
      inputStream.close()
    }.handleErrorWith { ex =>
      IO(println(s"Error closing HDFS file at path: $hdfsPath: $ex"))
    }
  }

  inputStreamResource.use { inputStream =>
    readInputStream(
      IO.pure(inputStream),
      chunkSize = 8192,
      blocker = blocker,       // Provide the blocker
      closeAfterUse = true     // Set to true or false as needed
    )
    .through(fs2.text.utf8Decode)
    .through(fs2.text.lines)
    .intersperse("\n")
    .compile
    .string
    .flatTap(content => IO(println(s"Finished reading HDFS file at path: $hdfsPath, content size: ${content.length}")))
  }
}

// Usage
val filePath = "hdfs://path/to/your/file"
val result: IO[String] = readHDFSFileLineByLine(filePath)
result.unsafeRunSync()

