dataProcessingConfig {
  sparkReadOptions {
    header = "true"
    inferSchema = "true"
    delimiter = ","
    # Add more read options as needed
  }
expressions = {
    controlColumn1 = "split(value, ',')[0]"
    controlColumn2 = "split(value, ',')[1]"
    # Add more expressions as needed
  }
}
===
val expressionsMap: Map[String, String] = dataProcessingConf.getConfig("expressions").entrySet().asScala.map { entry =>
  entry.getKey -> entry.getValue.unwrapped().toString
}.toMap

// Assuming `df` is the original DataFrame which has the 'value' column
val df: DataFrame = // Load your DataFrame here

val transformedDF: DataFrame = expressionsMap.foldLeft(df) { (tempDF, expression) =>
  tempDF.withColumn(expression._1, expr(expression._2))
}
===

import org.apache.spark.sql.{SparkSession, DataFrame}
import com.typesafe.config.ConfigFactory

import com.typesafe.config.ConfigFactory
import scala.collection.JavaConverters._

val conf = ConfigFactory.load()

// Get the hiveUserDefined Config object
val hiveUserDefinedConfig = conf.getConfig("sparkReadOptions.hiveUserDefined")

// Extract each entry as a key-value pair and convert it to a SQL expression
val columnExpressions: Seq[String] = hiveUserDefinedConfig.entrySet().asScala.toSeq.map { entry =>
  val key = entry.getKey
  val value = hiveUserDefinedConfig.getString(key)
  s"$key = $value"
}

// The rest of your code to apply these expressions to your DataFrame


// Assuming df is your original DataFrame which has the 'value' column
val df: DataFrame = // ... load your DataFrame

// Register the DataFrame as a temporary view
df.createOrReplaceTempView("temp_view")

// Construct the SQL query with the expressions
val sqlQuery = s"SELECT ${columnExpressions.mkString(", ")} FROM temp_view"

// Execute the SQL query
val transformedDF = spark.sql(sqlQuery)

// Show the result
transformedDF.show()

// Stop the Spark session
spark.stop()


df.withColumn("extractedValue", expr("regexp_extract(col1, '162648\\|(\\d+)', 1)")).show(false)
