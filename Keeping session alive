cd /path/to/framework_dq_materiality/
zip -r framework_dq_materiality.zip * 
cd C:\path\to\framework_dq_materiality
Compress-Archive -Path * -DestinationPath framework_dq_materiality.zip
pyspark --py-files /path/to/framework_dq_materiality.zip
# Test importing load_config from common
from common import load_config

# Load the configuration file
config = load_config("config/framework.conf")
print("Loaded Configuration:", config)



from pyspark.sql import SparkSession
from common import load_config
from common import SparkLogger  # Import the reusable Spark logger

def create_spark_session(app_name="SparkApplication"):
    """
    Creates and returns a Spark session.

    Args:
        app_name (str): Name of the Spark application.

    Returns:
        SparkSession: The Spark session object.
    """
    spark = SparkSession.builder \
        .appName(app_name) \
        .getOrCreate()
    return spark

def main():
    """
    Main entry point for the Spark job.
    """
    # Step 1: Initialize Spark Session
    spark = create_spark_session(app_name="FrameworkConfigJob")
    logger = SparkLogger(spark, log_name="FrameworkConfigLogger")

    try:
        logger.info("Spark Session initialized successfully.")
        
        # Step 2: Load the configuration
        config_file_path = "config/framework.conf"
        framework_config = load_config(config_file_path)
        logger.info("Framework Configuration Loaded Successfully.")
        logger.info(f"Loaded Configuration: {framework_config}")

        # Example: Access specific keys from a section
        output_table = framework_config["Output_data"]["Output_Table"]
        logger.info(f"Output Table: {output_table}")

        # Step 3: Example Spark logic
        data = [("Alice", 34), ("Bob", 45), ("Cathy", 29)]
        df = spark.createDataFrame(data, ["Name", "Age"])
        logger.info("Sample DataFrame created successfully.")
        df.show()

        logger.info("Spark Job Completed Successfully.")

    except Exception as e:
        logger.error(f"An error occurred: {e}")
        raise

    finally:
        # Step 4: Stop Spark session
        spark.stop()
        logger.info("Spark Session stopped successfully.")

if __name__ == "__main__":
    main()
