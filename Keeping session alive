import org.apache.spark.sql.SparkSession
import java.security.MessageDigest
import org.apache.spark.input.PortableDataStream

object CompactComputeHashes {
  def main(args: Array[String]): Unit = {
    val spark = SparkSession.builder.appName("Compact Compute Hashes").getOrCreate()
    val sc = spark.sparkContext
    val hdfsPath = "hdfs://path/to/your/files" // Update this path

    // Implicit class to convert PortableDataStream to Array[Byte]
    implicit class RichPortableDataStream(val pds: PortableDataStream) extends AnyVal {
      def toArray: Array[Byte] = {
        val inStream = pds.open()
        try {
          Stream.continually(inStream.read).takeWhile(_ != -1).map(_.toByte).toArray
        } finally {
          inStream.close()
        }
      }
    }

    // Function to compute MD5 hash from a PortableDataStream
    def computeMD5(pds: PortableDataStream): String = {
      val md = MessageDigest.getInstance("MD5")
      md.digest(pds.toArray).map("%02x".format(_)).mkString
    }

    // Read files, compute hashes, and collect
    val fileHashes = sc.binaryFiles(hdfsPath)
                       .mapValues(computeMD5)
                       .collect()

    // Generate master hash from concatenated file hashes
    val concatenatedHashes = fileHashes.map(_._2).mkString
    val masterHashBytes = MessageDigest.getInstance("MD5")
                                        .digest(concatenatedHashes.getBytes)
    val masterHash = masterHashBytes.map("%02x".format(_)).mkString

    println(s"Master Hash: $masterHash")

    spark.stop()
  }
}

