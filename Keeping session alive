import pandas as pd
import numpy as np
import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader
from sklearn.preprocessing import StandardScaler
from itertools import product

class LSTMAutoencoder(nn.Module):
    """
    LSTM-based Autoencoder for time series anomaly detection.

    Architecture:
    - Encoder: Compresses input sequences into latent representation
    - Decoder: Reconstructs original sequences from latent space

    Time Series Properties Handled:
    - Temporal dependencies: LSTM captures sequential patterns
    - Autocorrelation: Sequential nature preserves correlation structure
    - Seasonality: Model learns recurring patterns without explicit deseasonalization
    - Stationarity: NOT enforced - model learns from raw non-stationary data
    """
    def __init__(self, n_features, seq_length, hidden_size=64, dropout=0.2):
        super().__init__()
        self.seq_length = seq_length
        self.n_features = n_features

        print(f"Initializing LSTM Autoencoder:")
        print(f"  - Input features: {n_features}")
        print(f"  - Sequence length: {seq_length}")
        print(f"  - Hidden size: {hidden_size}")
        print(f"  - Dropout rate: {dropout}")

        # Encoder: Compress temporal sequences
        self.encoder_lstm1 = nn.LSTM(n_features, hidden_size, batch_first=True, dropout=dropout)
        self.encoder_lstm2 = nn.LSTM(hidden_size, hidden_size//2, batch_first=True, dropout=dropout)

        # Decoder: Reconstruct from compressed representation
        self.decoder_lstm1 = nn.LSTM(hidden_size//2, hidden_size, batch_first=True, dropout=dropout)
        self.decoder_lstm2 = nn.LSTM(hidden_size, n_features, batch_first=True, dropout=dropout)

    def forward(self, x):
        """
        Forward pass: Encode then decode input sequences
        Purpose: Learn to reconstruct normal patterns; fail on anomalies
        """
        # Encoder: Extract temporal features and compress
        encoded, _ = self.encoder_lstm1(x)  # First LSTM layer
        encoded, (hidden, cell) = self.encoder_lstm2(encoded)  # Compression layer

        # Bridge: Use final hidden state as compressed representation
        # Repeat for sequence length to enable reconstruction
        repeated = hidden[-1].unsqueeze(1).repeat(1, self.seq_length, 1)

        # Decoder: Reconstruct original sequence from compressed state
        decoded, _ = self.decoder_lstm1(repeated)  # Expand from compressed state
        decoded, _ = self.decoder_lstm2(decoded)  # Reconstruct to original dimensions

        return decoded

class TimeSeriesDataset(Dataset):
    """PyTorch Dataset wrapper for time series sequences"""
    def __init__(self, sequences):
        self.sequences = sequences
        print(f"Created dataset with {len(sequences)} sequences")

    def __len__(self):
        return len(self.sequences)

    def __getitem__(self, idx):
        return torch.FloatTensor(self.sequences[idx])

class LSTMAnomalyDetector:
    """
    LSTM Autoencoder for unsupervised anomaly detection in time series.

    Core Principle: Train on normal data to learn typical patterns.
    Anomalies produce higher reconstruction errors than normal data.

    Time Series Handling:
    - NO stationarity assumptions: Works with trending/seasonal data
    - Preserves autocorrelation: Sequential modeling maintains correlation structure
    - Learns seasonality: Recurring patterns captured without explicit deseasonalization
    - Handles multivariate: Multiple features processed simultaneously
    """

    def __init__(self, seq_length=30, hidden_size=64, dropout=0.2, lr=0.001, exclude_cols=None, max_grad_norm=1.0):
        """
        Initialize detector with hyperparameters

        Args:
            seq_length: Window size for temporal patterns (captures seasonality horizon)
            hidden_size: LSTM hidden dimensions (model capacity)
            dropout: Regularization to prevent overfitting
            lr: Learning rate for optimization
            exclude_cols: List of column names to exclude from training and prediction
            max_grad_norm: Maximum gradient norm for clipping (prevents explosion)
        """
        self.seq_length = seq_length
        self.hidden_size = hidden_size
        self.dropout = dropout
        self.lr = lr
        self.exclude_cols = exclude_cols or []
        self.max_grad_norm = max_grad_norm
        self.model = None
        self.scaler = StandardScaler()  # Handles different feature scales
        self.threshold = None
        self.device = torch.device('cpu')
        self.best_model_state = None  # FIXED: Store best model state in memory
        # self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

        print(f"LSTMAnomalyDetector initialized:")
        print(f"  - Device: {self.device}")
        print(f"  - Sequence length: {seq_length} (temporal window)")
        print(f"  - Hidden size: {hidden_size}")
        print(f"  - Learning rate: {lr}")
        print(f"  - Gradient clipping: {max_grad_norm} (prevents explosion)")
        if self.exclude_cols:
            print(f"  - Excluded columns: {self.exclude_cols}")
    def _validate_input_data(self, data, stage="training"):
        """
        Validate data quality before processing
        
        Checks for:
        - NaN values in rows (must be handled by user)
        - Infinite values (mathematical errors)
        """
        numeric_data = data.select_dtypes(include=[np.number])
        
        # Check for NaN in rows - REJECT (user must handle)
        nan_cols = numeric_data.columns[numeric_data.isnull().any()].tolist()
        if nan_cols:
            nan_counts = numeric_data[nan_cols].isnull().sum().to_dict()
            raise ValueError(
                f"NaN values detected in {len(nan_cols)} columns during {stage}:\n"
                f"{nan_counts}\n"
                f"handle missing values before passing data to the model.\n" #Options: df.fillna(), df.dropna(), or df.interpolate()
                
            )
        
        # Check for infinite values - REJECT
        inf_mask = np.isinf(numeric_data.values)
        if inf_mask.any():
            inf_cols = numeric_data.columns[np.isinf(numeric_data).any()].tolist()
            raise ValueError(
                f"Infinite values detected in columns during {stage}: {inf_cols}\n"
                f"Please handle infinite values before training."
            )
        
        return True
    
    def _filter_columns(self, data, is_training=True):
        """
        Filter columns: remove exclusions, all-NaN columns, and constant features
        
        Args:
            data: Input DataFrame
            is_training: If True, remove constant features. If False, use stored feature names.
        """
        if isinstance(data, pd.DataFrame):
            self._validate_input_data(data, stage="training" if is_training else "prediction")
            # Get numeric columns
            numeric_cols = data.select_dtypes(include=[np.number]).columns.tolist()
            
            # Apply user exclusions
            cols_to_use = [col for col in numeric_cols if col not in self.exclude_cols]
            X_subset = data[cols_to_use]
            
            # Remove all-NaN columns (edge case)
            X_subset = X_subset.dropna(axis=1, how='all')
            
            # Remove constant features (zero variance causes StandardScaler to fail)
            if is_training:
                n_unique = X_subset.nunique()
                non_constant_mask = n_unique > 1
                constant_cols = X_subset.columns[~non_constant_mask].tolist()
                
                if constant_cols:
                    print(f"Removed {len(constant_cols)} constant features (zero variance):")
                    for col in constant_cols[:5]:  # Show first 5
                        print(f"  - {col}")
                    if len(constant_cols) > 5:
                        print(f"  ... and {len(constant_cols) - 5} more")
                
                X_subset = X_subset.loc[:, non_constant_mask]
                self.feature_names_ = X_subset.columns.tolist()  # Store for predict()
            else:
                # During prediction, use same features as training
                if hasattr(self, 'feature_names_'):
                    missing_features = set(self.feature_names_) - set(X_subset.columns)
                    if missing_features:
                        raise ValueError(
                            f"Features missing in test data that were present in training: {missing_features}"
                        )
                    X_subset = X_subset[self.feature_names_]
            
            return X_subset
        else:
            return data

    def _create_sequences(self, data):
        """
        Create sliding window sequences from time series data

        Purpose: Convert time series into supervised learning format
        - Maintains temporal order (preserves autocorrelation)
        - Creates overlapping windows (captures all patterns)
        """
        sequences = []
        for i in range(len(data) - self.seq_length + 1):
            sequences.append(data[i:(i + self.seq_length)])

        print(f"Created {len(sequences)} sequences from {len(data)} data points")
        return np.array(sequences)

    def hyperparameter_search(self, X_train, param_grid, epochs=50):
        """
        Grid search for optimal hyperparameters

        Purpose: Find best configuration for your specific data patterns
        Returns best parameters based on validation reconstruction error
        """
        print("Starting hyperparameter search...")
        print(f"Parameter grid: {param_grid}")

        best_params = None
        best_score = float('inf')
        results = []

        # Generate all parameter combinations
        param_names = list(param_grid.keys())
        param_values = list(param_grid.values())

        for params in product(*param_values):
            param_dict = dict(zip(param_names, params))
            print(f"\nTesting parameters: {param_dict}")

            # Create temporary detector with current parameters
            temp_detector = LSTMAnomalyDetector(exclude_cols=self.exclude_cols, **param_dict)

            try:
                # Quick training for evaluation
                temp_detector.fit(X_train, epochs=epochs, validation_split=0.3, verbose=False)

                # Evaluate on validation data
                X_filtered = temp_detector._filter_columns(X_train, is_training=True) #X_filtered = temp_detector._filter_columns(X_train)
                X_scaled = temp_detector.scaler.transform(X_filtered)
                sequences = temp_detector._create_sequences(X_scaled)

                # Use subset for quick evaluation
                val_size = min(100, len(sequences) // 3)
                val_sequences = sequences[-val_size:]

                temp_detector.model.eval()
                with torch.no_grad():
                    val_tensor = torch.FloatTensor(val_sequences).to(temp_detector.device)
                    output = temp_detector.model(val_tensor)
                    val_loss = torch.mean((val_tensor - output) ** 2).item()

                results.append({**param_dict, 'val_loss': val_loss})
                print(f"Validation loss: {val_loss:.6f}")

                if val_loss < best_score:
                    best_score = val_loss
                    best_params = param_dict
                    print(f"New best parameters found!")

            except Exception as e:
                print(f"Error with parameters {param_dict}: {e}")
                continue

        print(f"\nHyperparameter search complete!")
        print(f"Best parameters: {best_params}")
        print(f"Best validation loss: {best_score:.6f}")

        # Update current detector with best parameters
        for key, value in best_params.items():
            setattr(self, key, value)

        return best_params, results

    def fit(self, X_train, epochs=100, batch_size=32, validation_split=0.2, verbose=True):
        """
        Train autoencoder on NORMAL data only (unsupervised learning)

        Purpose: Learn patterns of normal behavior
        - No target variable needed (autoencoder: input = target)
        - Learns to reconstruct typical patterns with low error
        - Will struggle to reconstruct anomalous patterns (high error)

        Time Series Considerations:
        - Preserves temporal order in sequences
        - StandardScaler handles different feature magnitudes
        - No detrending/deseasonalization (model learns these patterns)
        """
        print(f"\nTraining on {len(X_train)} normal data points...")
        print("Note: Training on NORMAL data only - no anomalies should be included")

        # Filter out excluded columns
        # X_filtered = self._filter_columns(X_train)
        X_filtered = self._filter_columns(X_train, is_training=True)
        print(f"Using {X_filtered.shape[1]} features after excluding {len(self.exclude_cols)} columns")

        # Scale features (important for LSTM convergence)
        print("Scaling features...")
        # X_scaled = self.scaler.fit_transform(X_filtered)
        # print(f"Feature scaling completed. Mean ~0, Std ~1 for all features")
        X_scaled = self.scaler.fit_transform(X_filtered)
        
        # CRITICAL: Validate no NaN created during scaling
        if np.isnan(X_scaled).any():
            raise ValueError(
                "NaN values created during scaling. This indicates:\n"
                "1. Features with zero variance (all identical values)\n"
                "2. Extreme outliers causing numerical instability \n"
            )
        
        print(f"Feature scaling completed. Mean ~0, Std ~1 for all features")

        # Create temporal sequences (sliding windows)
        print("Creating temporal sequences...")
        sequences = self._create_sequences(X_scaled)
        print(f"Sequence shape: {sequences.shape} [samples, time_steps, features]")

        # Split train/validation while preserving temporal order
        val_size = int(len(sequences) * validation_split)
        train_seq = sequences[:-val_size] if val_size > 0 else sequences
        val_seq = sequences[-val_size:] if val_size > 0 else sequences[:50]

        print(f"Training sequences: {len(train_seq)}")
        print(f"Validation sequences: {len(val_seq)}")

        # Create datasets and dataloaders
        train_dataset = TimeSeriesDataset(train_seq)
        val_dataset = TimeSeriesDataset(val_seq)
        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
        val_loader = DataLoader(val_dataset, batch_size=batch_size)

        # Initialize model
        print("Building LSTM Autoencoder model...")
        self.model = LSTMAutoencoder(
            X_filtered.shape[1], self.seq_length, self.hidden_size, self.dropout
        ).to(self.device)

        total_params = sum(p.numel() for p in self.model.parameters())
        print(f"Model has {total_params:,} trainable parameters")

        criterion = nn.MSELoss()  # Mean Squared Error for reconstruction
        optimizer = torch.optim.Adam(self.model.parameters(), lr=self.lr)
        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=5, factor=0.5)

        # Training loop with early stopping
        print(f"\nStarting training for up to {epochs} epochs...")
        best_val_loss = float('inf')
        patience_counter = 0

        for epoch in range(epochs):
            # Training phase
            self.model.train()
            train_loss = 0
            batch_count = 0

            for batch in train_loader:
                batch = batch.to(self.device)
                optimizer.zero_grad()

                # Forward pass: input = target for autoencoder
                output = self.model(batch)
                loss = criterion(output, batch)  # Reconstruction error

                # loss.backward()
                # optimizer.step()
                # train_loss += loss.item()
                # batch_count += 1
                loss.backward()
                
                # Gradient clipping to prevent explosion
                torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=self.max_grad_norm)
                
                optimizer.step()
                train_loss += loss.item()
                batch_count += 1

            # Validation phase
            self.model.eval()
            val_loss = 0
            val_batch_count = 0

            with torch.no_grad():
                for batch in val_loader:
                    batch = batch.to(self.device)
                    output = self.model(batch)
                    val_loss += criterion(output, batch).item()
                    val_batch_count += 1

            # Calculate average losses
            train_loss /= batch_count
            val_loss /= val_batch_count
            scheduler.step(val_loss)

            # Print progress
            if verbose and (epoch % 10 == 0 or epoch == epochs - 1):
                current_lr = optimizer.param_groups[0]['lr']
                print(f'Epoch {epoch:3d}: Train Loss: {train_loss:.6f}, '
                      f'Val Loss: {val_loss:.6f}, LR: {current_lr:.6f}')

            # FIXED: Early stopping - store state in memory instead of disk
            if val_loss < best_val_loss:
                best_val_loss = val_loss
                patience_counter = 0
                # Store state dict in memory (not disk)
                self.best_model_state = {k: v.cpu().clone() for k, v in self.model.state_dict().items()}
            else:
                patience_counter += 1
                if patience_counter >= 10:
                    print(f'Early stopping at epoch {epoch} (patience exceeded)')
                    break

        # FIXED: Load best model weights from memory instead of disk
        if self.best_model_state is not None:
            self.model.load_state_dict(self.best_model_state)
            print(f"Loaded best model with validation loss: {best_val_loss:.6f}")

        # Set anomaly threshold using validation reconstruction errors
        print("\nCalculating anomaly threshold...")
        self.model.eval()
        val_errors = []

        with torch.no_grad():
            for batch in val_loader:
                batch = batch.to(self.device)
                output = self.model(batch)
                # Calculate per-sequence reconstruction error
                mse = torch.mean((batch - output) ** 2, dim=(1, 2))
                val_errors.extend(mse.cpu().numpy())

        # Set threshold at 95th percentile of validation errors
        # Validate threshold calculation
        val_errors = np.array(val_errors)
        if len(val_errors) == 0 or np.isnan(val_errors).any():
            raise ValueError(
                "Cannot calculate anomaly threshold - validation errors contain NaN.\n"
                "gradient clipping not working properly."
            )
        
        # Set threshold at 95th percentile of validation errors
        self.threshold = np.percentile(val_errors, 95)
        print(f"Anomaly threshold set at 95th percentile: {self.threshold:.6f}")
        print("Sequences with reconstruction error > threshold will be flagged as anomalies")

        # Print threshold statistics
        mean_error = np.mean(val_errors)
        std_error = np.std(val_errors)
        print(f"Validation error statistics:")
        print(f"  - Mean: {mean_error:.6f}")
        print(f"  - Std: {std_error:.6f}")
        print(f"  - 95th percentile (threshold): {self.threshold:.6f}")

        return self

    def predict(self, X_test, date_col=None, batch_size=32, return_feature_errors=False):
        """
        Detect anomalies in test data with optional per-feature error analysis

        Purpose: Identify sequences with high reconstruction error
        - High error = pattern doesn't match learned normal behavior
        - Returns anomaly flags and confidence scores
        - Optional: Per-feature errors for interpretability

        Output:
        - reconstruction_error: MSE between input and reconstructed sequence
        - is_anomaly: Binary flag (error > threshold)
        - anomaly_score: Normalized score (error / threshold)
        - feature_errors: Per-feature reconstruction errors (if requested)

        Value of Per-Feature Errors:
        - **Root Cause Analysis**: Know which metrics are anomalous (CPU, memory, disk I/O)
        - **Targeted Investigation**: Focus on specific problematic profiling metrics
        - **Feature Importance**: Understand which features drive the anomaly decision
        """
        print(f"\nDetecting anomalies in {len(X_test)} test samples...")

        # Filter out excluded columns (same as training)
        # X_filtered = self._filter_columns(X_test)
        X_filtered = self._filter_columns(X_test, is_training=False)

        # Scale test data using training statistics (important!)
        print("Scaling test data using training statistics...")
        X_scaled = self.scaler.transform(X_filtered)
        feature_names = X_filtered.columns.tolist()

        # Create sequences
        print("Creating test sequences...")
        sequences = self._create_sequences(X_scaled)
        test_dataset = TimeSeriesDataset(sequences)
        test_loader = DataLoader(test_dataset, batch_size=batch_size)

        # Calculate reconstruction errors (both overall and per-feature)
        print("Computing reconstruction errors...")
        if return_feature_errors:
            print("  - Including per-feature error analysis for interpretability")

        self.model.eval()
        errors = []
        all_feature_errors = []
        processed_batches = 0

        with torch.no_grad():
            for batch in test_loader:
                batch = batch.to(self.device)
                output = self.model(batch)

                # Overall per-sequence MSE (reconstruction error)
                mse = torch.mean((batch - output) ** 2, dim=(1, 2))
                errors.extend(mse.cpu().numpy())

                # Per-feature reconstruction errors for interpretability
                # Shape: [batch_size, n_features] - average error across time steps per feature
                if return_feature_errors:
                    feature_errors = torch.mean((batch - output) ** 2, dim=1)  # [batch, features]
                    all_feature_errors.extend(feature_errors.cpu().numpy())

                processed_batches += 1

        errors = np.array(errors)
        print(f"Processed {processed_batches} batches, {len(errors)} sequences")

        # Apply threshold to identify anomalies
        anomaly_flags = errors > self.threshold
        anomaly_scores = errors / self.threshold  # Normalized confidence

        print(f"Anomaly detection results:")
        print(f"  - Total sequences: {len(errors)}")
        print(f"  - Anomalies detected: {np.sum(anomaly_flags)}")
        print(f"  - Anomaly rate: {np.mean(anomaly_flags)*100:.2f}%")
        print(f"  - Error range: [{np.min(errors):.6f}, {np.max(errors):.6f}]")

        # Create results DataFrame
        # Note: seq_length-1 rows lost due to sequence windowing
        start_idx = self.seq_length - 1
        results = pd.DataFrame({
            'reconstruction_error': errors,
            'is_anomaly': anomaly_flags,
            'anomaly_score': anomaly_scores
        })

        # Add per-feature errors for interpretability
        if return_feature_errors:
            all_feature_errors = np.array(all_feature_errors)
            print(f"Added per-feature error analysis for {len(feature_names)} features")

            # Add each feature's error as a separate column
            for i, feature_name in enumerate(feature_names):
                results[f'{feature_name}_error'] = all_feature_errors[:, i]

            # Identify top contributing features for each anomaly
            if np.any(anomaly_flags):
                print("Analyzing which features contribute most to anomalies...")
                anomaly_indices = np.where(anomaly_flags)[0]

                # Calculate mean feature errors for anomalous vs normal sequences
                normal_feature_errors = all_feature_errors[~anomaly_flags].mean(axis=0)
                anomaly_feature_errors = all_feature_errors[anomaly_flags].mean(axis=0)

                # Feature contribution ranking (anomaly error / normal error ratio)
                feature_contributions = anomaly_feature_errors / (normal_feature_errors + 1e-8)
                feature_ranking = sorted(zip(feature_names, feature_contributions),
                                       key=lambda x: x[1], reverse=True)

                print(f"\nTop features contributing to anomalies:")
                for i, (feature, contrib) in enumerate(feature_ranking[:5]):
                    print(f"  {i+1}. {feature}: {contrib:.2f}x higher error than normal")

        # Add date column if provided
        if date_col and date_col in X_test.columns:
            available_dates = X_test[date_col].iloc[start_idx:start_idx + len(errors)]
            results[date_col] = available_dates.reset_index(drop=True)
            print(f"Added date column: {date_col}")

        # Print top anomalies with feature breakdown
        if np.any(anomaly_flags):
            top_anomalies = results[results['is_anomaly']].nlargest(5, 'reconstruction_error')
            print(f"\nTop 5 anomalies:")

            for idx, row in top_anomalies.iterrows():
                date_str = f" on {row[date_col]}" if date_col in results.columns else ""
                print(f"  - Overall Error: {row['reconstruction_error']:.6f}, "
                      f"Score: {row['anomaly_score']:.2f}{date_str}")

                # Show which features had highest errors for this anomaly
                if return_feature_errors:
                    feature_errors_this_row = [(fname, row[f'{fname}_error'])
                                             for fname in feature_names]
                    top_features = sorted(feature_errors_this_row, key=lambda x: x[1], reverse=True)[:3]

                    print(f"    Top contributing features:")
                    for feat_name, feat_error in top_features:
                        print(f"      - {feat_name}: {feat_error:.6f}")

        return results
