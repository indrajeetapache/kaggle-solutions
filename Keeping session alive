from pyspark.sql import SparkSession
from common import SparkLogger  # Cleaner import

def create_spark_session(app_name="MySparkJob"):
    """
    Initialize and return a SparkSession.
    Args:
        app_name (str): Application name for Spark.
    Returns:
        SparkSession: Spark session instance.
    """
    return SparkSession.builder.appName(app_name).getOrCreate()

def process_data(spark, logger):
    """
    Main processing logic for the Spark job.
    Args:
        spark (SparkSession): Active Spark session.
        logger (SparkLogger): Logger for logging messages.
    """
    try:
        logger.info("Starting Spark Job...")

        # Sample data processing logic
        data = [("Alice", 34), ("Bob", 45), ("Cathy", 29)]
        df = spark.createDataFrame(data, ["Name", "Age"])
        logger.info("DataFrame created successfully.")

        # Perform some transformations (example)
        df = df.withColumnRenamed("Age", "UserAge")
        logger.info("Transformation completed: Renamed 'Age' to 'UserAge'.")

        df.show()
        logger.info("Spark Job completed successfully.")

    except Exception as e:
        logger.error(f"An error occurred during processing: {str(e)}")
        raise

def main():
    """
    Main entry point for the Spark job.
    """
    # Initialize Spark session
    spark = create_spark_session()

    # Initialize logger
    logger = SparkLogger(spark, log_name="MySparkDriver")

    # Call processing logic
    process_data(spark, logger)

    # Stop Spark session
    spark.stop()
    logger.info("Spark session stopped successfully.")

if __name__ == "__main__":
    main()
