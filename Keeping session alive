from pyspark.sql import SparkSession
from pyspark.sql import functions as F
from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType, DateType, BooleanType

def generate_customer_dataset(num_records: int,
                               database: str = "default",
                               table_name: str = "dim_customers",
                               mode: str = "overwrite"):
    """
    Generates a realistic customer dataset and saves it to a Hive table on Databricks.

    Args:
        num_records (int) : Number of records to generate (min: 1000)
        database    (str) : Target Hive database name       [default: 'default']
        table_name  (str) : Target Hive table name          [default: 'dim_customers']
        mode        (str) : Save mode - overwrite / append  [default: 'overwrite']
    """

    if num_records < 1000:
        raise ValueError(f"num_records must be at least 1000. Got: {num_records}")

    spark = SparkSession.builder \
        .appName("CustomerDatasetGenerator") \
        .getOrCreate()

    # ── Seed arrays (broadcast-friendly, used inside expr) ─────────────────────
    first_names = [
        "James","Mary","John","Patricia","Robert","Jennifer","Michael","Linda",
        "William","Barbara","David","Susan","Richard","Jessica","Joseph","Sarah",
        "Thomas","Karen","Charles","Lisa","Christopher","Nancy","Daniel","Betty",
        "Matthew","Margaret","Anthony","Sandra","Mark","Ashley","Donald","Emily",
        "Steven","Kimberly","Paul","Donna","Andrew","Carol","Joshua","Amanda",
        "Kevin","Melissa","Brian","Deborah","George","Stephanie","Edward","Rebecca",
        "Ronald","Sharon","Timothy","Laura","Jason","Cynthia","Jeffrey","Amy"
    ]

    last_names = [
        "Smith","Johnson","Williams","Brown","Jones","Garcia","Miller","Davis",
        "Rodriguez","Martinez","Hernandez","Lopez","Gonzalez","Wilson","Anderson",
        "Thomas","Taylor","Moore","Jackson","Martin","Lee","Perez","Thompson","White",
        "Harris","Sanchez","Clark","Ramirez","Lewis","Robinson","Walker","Young",
        "Allen","King","Wright","Scott","Torres","Nguyen","Hill","Flores","Green",
        "Adams","Nelson","Baker","Hall","Rivera","Campbell","Mitchell","Carter","Roberts"
    ]

    domains     = ["gmail.com","yahoo.com","outlook.com","hotmail.com","icloud.com","protonmail.com"]
    countries   = ["USA","Canada","UK","Australia","Germany","France","India","Singapore","Brazil","Japan"]
    cities      = ["New York","Los Angeles","Chicago","Houston","Phoenix","Toronto","London","Sydney",
                   "Berlin","Paris","Mumbai","Singapore","São Paulo","Tokyo","Dubai","Amsterdam"]
    segments    = ["Premium","Standard","Basic","Enterprise","Trial"]
    genders     = ["Male","Female","Non-Binary","Prefer Not to Say"]
    card_types  = ["Visa","Mastercard","Amex","Discover","PayPal"]
    statuses    = ["Active","Inactive","Suspended","Pending"]

    # Register arrays as SQL-accessible via spark config trick → use F.array() instead
    fn_arr      = F.array([F.lit(x) for x in first_names])
    ln_arr      = F.array([F.lit(x) for x in last_names])
    domain_arr  = F.array([F.lit(x) for x in domains])
    country_arr = F.array([F.lit(x) for x in countries])
    city_arr    = F.array([F.lit(x) for x in cities])
    segment_arr = F.array([F.lit(x) for x in segments])
    gender_arr  = F.array([F.lit(x) for x in genders])
    card_arr    = F.array([F.lit(x) for x in card_types])
    status_arr  = F.array([F.lit(x) for x in statuses])

    def rand_pick(arr):
        """Pick a random element from a Spark array column expression."""
        return arr[F.floor(F.rand() * F.size(arr)).cast(IntegerType())]

    # ── Build DataFrame using range (most efficient for large data on Databricks) ─
    df = (
        spark.range(1, num_records + 1)                         # customer_id: 1 … N
        .withColumnRenamed("id", "customer_id")

        # Name fields
        .withColumn("first_name",   rand_pick(fn_arr))
        .withColumn("last_name",    rand_pick(ln_arr))
        .withColumn("full_name",    F.concat_ws(" ", "first_name", "last_name"))

        # Contact
        .withColumn("email",
            F.concat_ws("",
                F.lower("first_name"), F.lit("."), F.lower("last_name"),
                F.lit("_"), (F.rand() * 999 + 1).cast(IntegerType()).cast(StringType()),
                F.lit("@"), rand_pick(domain_arr)
            )
        )
        .withColumn("phone_number",
            F.concat_ws("-",
                (F.rand() * 899 + 100).cast(IntegerType()).cast(StringType()),
                (F.rand() * 899 + 100).cast(IntegerType()).cast(StringType()),
                (F.rand() * 8999 + 1000).cast(IntegerType()).cast(StringType())
            )
        )

        # Demographics
        .withColumn("gender",       rand_pick(gender_arr))
        .withColumn("age",          (F.rand() * 57 + 18).cast(IntegerType()))  # 18–74
        .withColumn("date_of_birth",
            F.date_sub(F.current_date(), (F.col("age") * 365 + F.rand() * 364).cast(IntegerType()))
        )

        # Geography
        .withColumn("country",      rand_pick(country_arr))
        .withColumn("city",         rand_pick(city_arr))
        .withColumn("zip_code",
            F.lpad((F.rand() * 89999 + 10000).cast(IntegerType()).cast(StringType()), 5, "0")
        )

        # Account
        .withColumn("customer_segment",  rand_pick(segment_arr))
        .withColumn("account_status",    rand_pick(status_arr))
        .withColumn("registration_date",
            F.date_sub(F.current_date(), (F.rand() * 3650).cast(IntegerType()))   # within last 10 yrs
        )
        .withColumn("last_login_date",
            F.date_sub(F.current_date(), (F.rand() * 365).cast(IntegerType()))
        )
        .withColumn("is_email_verified",  (F.rand() > 0.2).cast(BooleanType()))   # 80% verified
        .withColumn("is_sms_opt_in",      (F.rand() > 0.5).cast(BooleanType()))

        # Financials
        .withColumn("annual_income",        F.round(F.rand() * 190000 + 20000, 2))   # $20k–$210k
        .withColumn("credit_score",         (F.rand() * 550 + 300).cast(IntegerType()))  # 300–850
        .withColumn("total_orders",         (F.rand() * 200).cast(IntegerType()))
        .withColumn("total_spend_usd",      F.round(F.rand() * 49900 + 100, 2))      # $100–$50k
        .withColumn("avg_order_value_usd",
            F.round(F.when(F.col("total_orders") > 0,
                           F.col("total_spend_usd") / F.col("total_orders"))
                    .otherwise(0), 2)
        )
        .withColumn("preferred_payment",    rand_pick(card_arr))
        .withColumn("loyalty_points",       (F.rand() * 10000).cast(IntegerType()))

        # Metadata
        .withColumn("created_at",  F.current_timestamp())
        .withColumn("updated_at",  F.current_timestamp())
    )

    # ── Save to Hive ────────────────────────────────────────────────────────────
    spark.sql(f"CREATE DATABASE IF NOT EXISTS {database}")

    (
        df.write
        .format("delta")           # Delta Lake — best practice on Databricks
        .mode(mode)
        .option("overwriteSchema", "true")
        .saveAsTable(f"{database}.{table_name}")
    )

    count = spark.table(f"{database}.{table_name}").count()
    print(f"✅ Successfully saved {count:,} records → {database}.{table_name}")
    spark.table(f"{database}.{table_name}").printSchema()


# ── Usage ───────────────────────────────────────────────────────────────────────
generate_customer_dataset(num_records=5000)

# Custom DB / table / append mode
# generate_customer_dataset(num_records=10000, database="marketing", table_name="dim_customers", mode="overwrite")
====================================================


# ============================================================
# PHOTON vs NON-PHOTON BENCHMARKING NOTEBOOK
# Run this SAME notebook on Cluster A (Photon) & Cluster B (No Photon)
# ============================================================

from pyspark.sql import SparkSession
from pyspark.sql import functions as F
from pyspark.sql.types import IntegerType
import time

spark = SparkSession.builder.appName("PhotonBenchmark").getOrCreate()

is_photon = "photon" in spark.conf.get("spark.databricks.clusterUsageTags.runtimeEngine", "").lower()
run_label = "PHOTON" if is_photon else "NON-PHOTON"
print(f"Running on: {run_label}")

# ============================================================
# STEP 1: Generate Dataset
# ============================================================
def generate_customer_dataset(num_records: int):
    first_names = ["James","Mary","John","Patricia","Robert","Jennifer","Michael","Linda",
                   "William","Barbara","David","Susan","Richard","Jessica","Joseph","Sarah",
                   "Thomas","Karen","Charles","Lisa","Christopher","Nancy","Daniel","Betty",
                   "Matthew","Margaret","Anthony","Sandra","Mark","Ashley","Donald","Emily",
                   "Steven","Kimberly","Paul","Donna","Andrew","Carol","Joshua","Amanda"]

    last_names  = ["Smith","Johnson","Williams","Brown","Jones","Garcia","Miller","Davis",
                   "Rodriguez","Martinez","Hernandez","Lopez","Gonzalez","Wilson","Anderson",
                   "Thomas","Taylor","Moore","Jackson","Martin","Lee","Perez","Thompson"]

    domains     = ["gmail.com","yahoo.com","outlook.com","hotmail.com","icloud.com"]
    countries   = ["USA","Canada","UK","Australia","Germany","France","India","Singapore"]
    segments    = ["Premium","Standard","Basic","Enterprise","Trial"]
    statuses    = ["Active","Inactive","Suspended","Pending"]

    fn_arr      = F.array([F.lit(x) for x in first_names])
    ln_arr      = F.array([F.lit(x) for x in last_names])
    domain_arr  = F.array([F.lit(x) for x in domains])
    country_arr = F.array([F.lit(x) for x in countries])
    segment_arr = F.array([F.lit(x) for x in segments])
    status_arr  = F.array([F.lit(x) for x in statuses])

    def rand_pick(arr):
        return arr[F.floor(F.rand() * F.size(arr)).cast(IntegerType())]

    return (
        spark.range(1, num_records + 1)
        .withColumnRenamed("id", "customer_id")
        .withColumn("first_name",        rand_pick(fn_arr))
        .withColumn("last_name",         rand_pick(ln_arr))
        .withColumn("full_name",         F.concat_ws(" ", "first_name", "last_name"))
        .withColumn("email",             F.concat_ws("", F.lower("first_name"), F.lit("."),
                                         F.lower("last_name"), F.lit("@"), rand_pick(domain_arr)))
        .withColumn("age",               (F.rand() * 57 + 18).cast(IntegerType()))
        .withColumn("country",           rand_pick(country_arr))
        .withColumn("customer_segment",  rand_pick(segment_arr))
        .withColumn("account_status",    rand_pick(status_arr))
        .withColumn("annual_income",     F.round(F.rand() * 190000 + 20000, 2))
        .withColumn("credit_score",      (F.rand() * 550 + 300).cast(IntegerType()))
        .withColumn("total_orders",      (F.rand() * 200).cast(IntegerType()))
        .withColumn("total_spend_usd",   F.round(F.rand() * 49900 + 100, 2))
        .withColumn("registration_date", F.date_sub(F.current_date(), (F.rand() * 3650).cast(IntegerType())))
        .withColumn("is_email_verified", (F.rand() > 0.2))
        .withColumn("loyalty_points",    (F.rand() * 10000).cast(IntegerType()))
    )

NUM_RECORDS = 5_000_000

print(f"Generating {NUM_RECORDS:,} records...")
t0 = time.time()
df = generate_customer_dataset(NUM_RECORDS)
df.write.format("delta").mode("overwrite").option("overwriteSchema","true") \
  .saveAsTable("hive_metastore.default.bench_customers")
print(f"Dataset saved — {round(time.time()-t0, 3)}s")

df = spark.table("hive_metastore.default.bench_customers")
df.cache()
df.count()  # warm up cache
print("Cache warmed up\n")

# ============================================================
# BENCHMARK HELPER
# ============================================================
results = {}

def benchmark(name, func):
    # Cold run (discard)
    func()
    # Timed runs (average of 3)
    times = []
    for i in range(3):
        print(f"  [{name}] Run {i+1} — START  | timestamp: {time.strftime('%H:%M:%S')}")
        start = time.time()
        func()
        elapsed = round(time.time() - start, 3)
        times.append(elapsed)
        print(f"  [{name}] Run {i+1} — END    | duration: {elapsed}s")
    avg = round(sum(times) / len(times), 3)
    results[name] = avg
    print(f"  [{name}] Average: {avg}s  (all runs: {[round(t,2) for t in times]})\n")
    return avg

print("=" * 60)
print(f"  BENCHMARK SUITE — {run_label}")
print("=" * 60 + "\n")

# ============================================================
# TEST 1: Large Aggregation
# ============================================================
print("TEST 1: Large Aggregation — START")
t_start = time.time()

def test_aggregation():
    spark.table("hive_metastore.default.bench_customers") \
        .groupBy("customer_segment", "country", "account_status") \
        .agg(
            F.count("*").alias("total_customers"),
            F.avg("annual_income").alias("avg_income"),
            F.avg("credit_score").alias("avg_credit"),
            F.sum("total_spend_usd").alias("total_spend"),
            F.max("total_orders").alias("max_orders"),
            F.stddev("annual_income").alias("stddev_income")
        ).collect()

benchmark("GroupBy + Multi-Aggregation", test_aggregation)
print(f"TEST 1: Large Aggregation — END | total elapsed: {round(time.time()-t_start,3)}s\n")

# ============================================================
# TEST 2: Filter + Projection
# ============================================================
print("TEST 2: Filter + Projection — START")
t_start = time.time()

def test_filter():
    spark.table("hive_metastore.default.bench_customers") \
        .filter(
            (F.col("annual_income") > 80000) &
            (F.col("credit_score") > 650) &
            (F.col("account_status") == "Active") &
            (F.col("is_email_verified") == True)
        ) \
        .select("customer_id","full_name","email","annual_income","credit_score","total_spend_usd") \
        .collect()

benchmark("Filter + Projection (4 conditions)", test_filter)
print(f"TEST 2: Filter + Projection — END | total elapsed: {round(time.time()-t_start,3)}s\n")

# ============================================================
# TEST 3: Join
# ============================================================
print("TEST 3: Large Join — START")
t_start = time.time()

def test_join():
    df_main = spark.table("hive_metastore.default.bench_customers").alias("main")
    df_ref  = spark.table("hive_metastore.default.bench_customers") \
                   .filter(F.col("customer_segment") == "Premium").alias("ref")
    df_main.join(df_ref, F.col("main.country") == F.col("ref.country"), "inner") \
           .select("main.customer_id","main.full_name","ref.customer_segment","ref.annual_income") \
           .limit(500000) \
           .collect()

benchmark("Inner Join (country match)", test_join)
print(f"TEST 3: Large Join — END | total elapsed: {round(time.time()-t_start,3)}s\n")

# ============================================================
# TEST 4: Window Functions
# ============================================================
print("TEST 4: Window Functions — START")
t_start = time.time()

def test_window():
    from pyspark.sql.window import Window
    w = Window.partitionBy("customer_segment").orderBy(F.desc("total_spend_usd"))
    spark.table("hive_metastore.default.bench_customers") \
        .withColumn("rank",           F.rank().over(w)) \
        .withColumn("dense_rank",     F.dense_rank().over(w)) \
        .withColumn("running_total",  F.sum("total_spend_usd").over(w.rowsBetween(Window.unboundedPreceding, 0))) \
        .withColumn("pct_of_segment", F.col("total_spend_usd") / F.sum("total_spend_usd").over(Window.partitionBy("customer_segment"))) \
        .filter(F.col("rank") <= 1000) \
        .collect()

benchmark("Window Rank + Running Total", test_window)
print(f"TEST 4: Window Functions — END | total elapsed: {round(time.time()-t_start,3)}s\n")

# ============================================================
# TEST 5: String Operations
# ============================================================
print("TEST 5: String Operations — START")
t_start = time.time()

def test_string():
    spark.table("hive_metastore.default.bench_customers") \
        .withColumn("email_domain",  F.split("email", "@").getItem(1)) \
        .withColumn("name_upper",    F.upper("full_name")) \
        .withColumn("name_length",   F.length("full_name")) \
        .withColumn("email_masked",  F.regexp_replace("email", r"(?<=.).(?=[^@]*@)", "*")) \
        .groupBy("email_domain") \
        .agg(F.count("*"), F.avg("name_length")) \
        .collect()

benchmark("String Ops + Regex + GroupBy", test_string)
print(f"TEST 5: String Operations — END | total elapsed: {round(time.time()-t_start,3)}s\n")

# ============================================================
# TEST 6: Sort
# ============================================================
print("TEST 6: Sort — START")
t_start = time.time()

def test_sort():
    spark.table("hive_metastore.default.bench_customers") \
        .orderBy(F.desc("total_spend_usd"), F.asc("credit_score"), F.desc("annual_income")) \
        .limit(100000) \
        .collect()

benchmark("Multi-column Sort + Limit", test_sort)
print(f"TEST 6: Sort — END | total elapsed: {round(time.time()-t_start,3)}s\n")

# ============================================================
# FINAL RESULTS TABLE
# ============================================================
print("=" * 60)
print(f"  FINAL RESULTS — {run_label}")
print("=" * 60)
print(f"  {'Test':<40} {'Avg Time (s)':>12}")
print(f"  {'-'*52}")
for test, duration in results.items():
    print(f"  {test:<40} {duration:>12.3f}s")
print(f"\n  Total time: {sum(results.values()):.3f}s")
print("=" * 60)

# Save results to Delta for comparison query
results_df = spark.createDataFrame(
    [(run_label, k, v) for k, v in results.items()],
    ["cluster_type", "test_name", "avg_seconds"]
)
results_df.write.format("delta").mode("append") \
          .saveAsTable("hive_metastore.default.photon_benchmark_results")
print("Results saved to hive_metastore.default.photon_benchmark_results")
