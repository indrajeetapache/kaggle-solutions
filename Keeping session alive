dataProcessingConfig {
  sparkReadOptions {
    header = "true"
    inferSchema = "true"
    delimiter = ","
    # Add more read options as needed
  }
expressions = {
    controlColumn1 = "split(value, ',')[0]"
    controlColumn2 = "split(value, ',')[1]"
    # Add more expressions as needed
  }
}
===
val expressionsMap: Map[String, String] = dataProcessingConf.getConfig("expressions").entrySet().asScala.map { entry =>
  entry.getKey -> entry.getValue.unwrapped().toString
}.toMap

// Assuming `df` is the original DataFrame which has the 'value' column
val df: DataFrame = // Load your DataFrame here

val transformedDF: DataFrame = expressionsMap.foldLeft(df) { (tempDF, expression) =>
  tempDF.withColumn(expression._1, expr(expression._2))
}
