cd /path/to/framework_dq_materiality/
zip -r framework_dq_materiality.zip * 
cd C:\path\to\framework_dq_materiality
Compress-Archive -Path * -DestinationPath framework_dq_materiality.zip
pyspark --py-files /path/to/framework_dq_materiality.zip
# Test importing load_config from common
from common import load_config

# Load the configuration file
config = load_config("config/framework.conf")
print("Loaded Configuration:", config)

from common.utils import load_config
from common.log import get_logger

logger = get_logger("PySparkTestLogger")

config_file_path = "config/framework.conf"
config_data = load_config(config_file_path)
logger.info(f"Configuration Loaded: {config_data}")

Get-ChildItem *.zip


import sys
import os

# Append the path of the ZIP file to PYTHONPATH
zip_file_path = "framework_dq_materiality.zip"
sys.path.append(os.path.abspath(zip_file_path))

# Verify the path is added
print(sys.path)

===
# Initialize the logger
logger = get_logger("TestLogger")
logger.info("Logger initialized successfully.")

# Load the configuration
config_file_path = "config/framework.conf"
config = load_config(config_file_path)
logger.info(f"Loaded Config: {config}")
==========

def get_sample_dict():
    """
    Returns a sample dictionary for testing.
    """
    return {"key1": "value1", "key2": "value2", "key3": "value3"}
from .sample_data import get_sample_dict

========================

from pyspark.sql import SparkSession
from pyspark.sql.functions import col, lit
from common.log import get_logger

logger = get_logger("FailedRecordTransform")

def transform_failed_records(spark, input_path):
    """
    Perform transformations on the Failed Records Table.
    Args:
        spark (SparkSession): The Spark session.
        input_path (str): Path to the input Failed Records Table.
    Returns:
        DataFrame: Transformed Failed Records Table.
    """
    logger.info("Starting transformation for Failed Records Table")
    df = spark.read.parquet(input_path)
    
    # Example transformation: Add a 'status' column for failed records
    transformed_df = df.withColumn("record_status", lit("FAILED")) \
                       .filter(col("error_code").isNotNull())

    logger.info("Transformation completed for Failed Records Table")
    return transformed_df


        return config_dict

    except Exception as e:
        logger.error(f"Error loading configuration: {e}")
        raise RuntimeError("Failed to load configuration") from e
