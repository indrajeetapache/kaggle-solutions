def computeMD5(hdfsPath: String, bufferSizeMB: Int, spark: SparkSession): (String, Double) = {
  val bufferSize = bufferSizeMB * 1024 * 1024
  
  logger.info(s"Starting MD5 calculation with ${bufferSizeMB}MB buffer size for file: $hdfsPath")
  
  val startTime = System.nanoTime()
  
  val fs = FileSystem.get(spark.sparkContext.hadoopConfiguration)
  val rawInputStream = fs.open(new Path(hdfsPath))
  val inputStream = new BufferedInputStream(rawInputStream, bufferSize)
  
  val fileSize = fs.getFileStatus(new Path(hdfsPath)).getLen
  val totalMB = fileSize / (1024 * 1024) // Add this line to define totalMB
  val md = MessageDigest.getInstance("MD5")
  val buffer = new Array[Byte](bufferSize)
  
  var bytesRead = 0
  var totalBytesRead = 0L
  var lastReportTime = System.nanoTime()
  val reportIntervalMs = 10000 // Report every 10 seconds
  
  try {
    while ({bytesRead = inputStream.read(buffer); bytesRead != -1}) {
      md.update(buffer, 0, bytesRead)
      totalBytesRead += bytesRead
      
      // Progress reporting
      val currentTime = System.nanoTime()
      if ((currentTime - lastReportTime) / 1000000 > reportIntervalMs) {
        val percent = (totalBytesRead.toDouble / fileSize) * 100
        val elapsedSec = (currentTime - startTime) / 1e9
        val mbProcessed = totalBytesRead / (1024 * 1024)
        val mbPerSec = mbProcessed / elapsedSec
        
        logger.info(f"Progress: $percent%.2f%% ($mbProcessed/$totalMB MB), Speed: $mbPerSec%.2f MB/s")
        
        lastReportTime = currentTime
      }
    }
  } finally {
    try {
      inputStream.close()
    } catch {
      case e: Exception => logger.warn(s"Error closing input stream: ${e.getMessage}")
    }
  }
  
  val md5Hash = md.digest().map("%02x".format(_)).mkString
  val endTime = System.nanoTime()
  val duration = (endTime - startTime) / 1e9 // Convert to seconds
  
  logger.info(f"MD5 hash: $md5Hash (buffer size: ${bufferSizeMB}MB, time: $duration%.3f seconds)")
  
  (md5Hash, duration)
}
