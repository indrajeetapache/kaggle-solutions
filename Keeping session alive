from pyspark.sql import SparkSession
from pyspark.sql.functions import *
import time

# ══════════════════════════════════════════════════════════════
# CONFIGURATION
# ══════════════════════════════════════════════════════════════
NUM_ROWS = 8_000_000
DATABASE = "test_db"                    # Change to your database
TABLE_NAME = "transactions_8m"
FULL_TABLE = f"{DATABASE}.{TABLE_NAME}"

# ══════════════════════════════════════════════════════════════
# INITIALIZE SPARK
# ══════════════════════════════════════════════════════════════
spark = SparkSession.builder \
    .appName("8M_Row_Benchmark") \
    .enableHiveSupport() \
    .getOrCreate()

# Create database if not exists
spark.sql(f"CREATE DATABASE IF NOT EXISTS {DATABASE}")
spark.sql(f"USE {DATABASE}")

print("=" * 70)
print("STEP 1: GENERATE 8 MILLION ROWS")
print("=" * 70)

start_gen = time.time()

# Generate 8M rows
df = spark.range(0, NUM_ROWS, numPartitions=80) \
    .withColumn("transaction_id", col("id")) \
    .withColumn("customer_id", (rand() * 1_000_000).cast("bigint")) \
    .withColumn("product_id", (rand() * 10_000).cast("int")) \
    .withColumn("store_id", (rand() * 500).cast("int")) \
    .withColumn("transaction_date", date_add(lit("2024-01-01"), (rand() * 365).cast("int"))) \
    .withColumn("quantity", (rand() * 10 + 1).cast("int")) \
    .withColumn("unit_price", (rand() * 500 + 10).cast("decimal(10,2)")) \
    .withColumn("total_amount", col("quantity") * col("unit_price")) \
    .withColumn("payment_method", 
        when(rand() < 0.4, "CREDIT_CARD")
        .when(rand() < 0.7, "DEBIT_CARD")
        .when(rand() < 0.9, "CASH")
        .otherwise("UPI")) \
    .withColumn("status", 
        when(rand() < 0.95, "COMPLETED").otherwise("CANCELLED")) \
    .withColumn("region", 
        when(rand() < 0.25, "NORTH")
        .when(rand() < 0.50, "SOUTH")
        .when(rand() < 0.75, "EAST")
        .otherwise("WEST")) \
    .drop("id")

# Write to Hive table (partitioned by transaction_date)
df.write \
    .mode("overwrite") \
    .format("parquet") \
    .partitionBy("transaction_date") \
    .saveAsTable(FULL_TABLE)

gen_time = time.time() - start_gen
print(f"\n✓ Data written to table: {FULL_TABLE}")
print(f"✓ Time taken: {gen_time:.2f} seconds")

# ══════════════════════════════════════════════════════════════
# STEP 2: VERIFY DATA
# ══════════════════════════════════════════════════════════════
print("\n" + "=" * 70)
print("STEP 2: VERIFY DATA")
print("=" * 70)

row_count = spark.sql(f"SELECT COUNT(*) FROM {FULL_TABLE}").collect()[0][0]
print(f"\n✓ Row count: {row_count:,}")

# Show table info
spark.sql(f"DESCRIBE EXTENDED {FULL_TABLE}").show(50, truncate=False)

# Show sample data
print("\nSample data:")
spark.sql(f"SELECT * FROM {FULL_TABLE} LIMIT 5").show(truncate=False)

# Show partition count
partition_count = spark.sql(f"SHOW PARTITIONS {FULL_TABLE}").count()
print(f"\n✓ Partitions: {partition_count}")

# ══════════════════════════════════════════════════════════════
# STEP 3: RUN BENCHMARK JOB
# ══════════════════════════════════════════════════════════════
print("\n" + "=" * 70)
print("STEP 3: BENCHMARK JOB (Aggregation + Join)")
print("=" * 70)

start_job = time.time()

# Create temporary customer dimension table
df_customers = spark.range(0, 1_000_000) \
    .withColumnRenamed("id", "customer_id") \
    .withColumn("customer_segment", 
        when(rand() < 0.3, "PREMIUM")
        .when(rand() < 0.7, "STANDARD")
        .otherwise("BASIC"))

df_customers.createOrReplaceTempView("customers_temp")

# Benchmark SQL query
result = spark.sql(f"""
    WITH aggregated AS (
        SELECT 
            customer_id,
            region,
            payment_method,
            SUM(total_amount) as total_spent,
            COUNT(transaction_id) as txn_count
        FROM {FULL_TABLE}
        WHERE status = 'COMPLETED'
        GROUP BY customer_id, region, payment_method
    )
    SELECT 
        a.region,
        c.customer_segment,
        a.payment_method,
        SUM(a.total_spent) as revenue,
        SUM(a.txn_count) as transactions,
        COUNT(DISTINCT a.customer_id) as customers
    FROM aggregated a
    JOIN customers_temp c ON a.customer_id = c.customer_id
    GROUP BY a.region, c.customer_segment, a.payment_method
    ORDER BY revenue DESC
""")

# Trigger execution
output_rows = result.count()
result.show(20, truncate=False)

job_time = time.time() - start_job

# ══════════════════════════════════════════════════════════════
# STEP 4: RESULTS
# ══════════════════════════════════════════════════════════════
print("\n" + "=" * 70)
print("BENCHMARK RESULTS")
print("=" * 70)
print(f"Platform:        {spark.sparkContext.master}")
print(f"Table:           {FULL_TABLE}")
print(f"Input Rows:      {row_count:,}")
print(f"Output Rows:     {output_rows:,}")
print(f"Job Time:        {job_time:.2f} seconds ({job_time/60:.2f} minutes)")
print("=" * 70)

print(f"""
NEXT STEPS:
1. Check Spark UI for detailed metrics
2. Run same script on Databricks (change DATABASE name if needed)
3. Compare job_time between platforms

TABLE INFO:
- Database: {DATABASE}
- Table: {TABLE_NAME}
- Format: Parquet
- Partitions: {partition_count} (by transaction_date)
- Expected size: ~240 MB
""")

spark.stop()
