import org.apache.spark.sql.functions._

// Processing logic in one shot
val processedDataFrame = rawDataFrame
  .filter(col("unique_id") >= lit(headerRowsToIgnore)) // Filter header rows
  .filter(col("unique_id") < lit(totalRows - footerRowsToIgnore)) // Filter footer rows
  .withColumn(
    "line",
    when(
      lit(isMultiDelimiter), // Check if isMultiDelimiter is true
      regexp_replace(col("line"), delimiterPattern, effectiveReplacementChar.toString) // Replace with effectiveReplacementChar
    ).otherwise(
      regexp_replace(col("line"), delimiterPattern, delimiterPattern) // Replace with same delimiterPattern
    )
  )
  .withColumn("isMultiDelimiter", lit(isMultiDelimiter)) // Add column to indicate if multi-delimiter is used
  .drop("unique_id") // Drop unique_id column if not needed

// Define the schema and split logic
val delimiter = optionsMap("delimiter")
val columnNames = dfSchema.fieldNames.filter(_ != "_corrupt_record")

val finalDataFrame = processedDataFrame
  .withColumn("split", split(col("line"), delimiter)) // Split line into array based on delimiter
  .select((0 until columnNames.length).map(i => col("split").getItem(i).alias(columnNames(i))): _*) // Map array to columns
  .withColumn("_corrupt_record", lit(null).cast(StringType)) // Add corrupt column as null
  .withColumn(
    "source_file_name",
    substring_index(input_file_name(), "/", -1) // Extract file name
  )
  .withColumn(
    "source_file_name_with_path",
    input_file_name() // Extract full file path
  )

// Show and print schema of the final DataFrame
finalDataFrame.show(false)
finalDataFrame.printSchema()
