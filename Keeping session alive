package com.citi.fileingestionframework.driver

// Standard imports
import java.io.{BufferedInputStream, File}
import java.security.MessageDigest
import java.nio.file.Paths

// Apache imports
import org.apache.hadoop.fs.{FileSystem, Path}
import org.apache.spark.sql.SparkSession
import org.apache.log4j.{Level, Logger}

// Google Guava imports
import com.google.common.hash.Hashing
import com.google.common.io.{Files, ByteSource}

// Scala imports
import scala.collection.mutable.ArrayBuffer
import java.util.concurrent.{Executors, LinkedBlockingQueue}
import scala.concurrent.{ExecutionContext, Future, Await}
import scala.concurrent.duration._

/**
 * MD5 benchmark utility that tests and compares different MD5 calculation methods
 * for HDFS files.
 */
object MD5DeepBenchmark {
  // Initialize logger
  val logger = Logger.getLogger(this.getClass)

  /**
   * Thread-safe container for chunk data in the parallel processing approach.
   * Crucial for maintaining proper ordering of chunks during parallel reads.
   */
  case class ChunkData(index: Long, data: Array[Byte], size: Int)
  
  /**
   * Main method to run benchmarks
   */
  def main(args: Array[String]): Unit = {
    // Configure logging
    Logger.getLogger("org").setLevel(Level.WARN)
    Logger.getLogger("akka").setLevel(Level.WARN)
    
    println("MD5 Benchmark starting")
    
    val spark = SparkSession.builder()
      .appName("MD5 Buffer Size Benchmark")
      .getOrCreate()
      
    try {
      if (args.length < 1) {
        println("Usage: MD5DeepBenchmark <hdfs_file_path> [buffer_size_1,buffer_size_2,...]")
        System.exit(1)
      }
      
      val hdfsPath = args(0)
      
      // Parse buffer sizes to test
      val defaultSizes = Array(4, 8, 16)
      val bufferSizes = if (args.length > 1) {
        args(1).split(",").map(_.trim.toInt)
      } else {
        defaultSizes
      }
      
      logger.info(s"Testing buffer sizes (MB): ${bufferSizes.mkString(", ")}")
      
      // Run benchmark
      val results = benchmarkMD5(hdfsPath, spark, bufferSizes)
      
      // Find optimal buffer size (fastest)
      val optimalSize = results.minBy(_._2._2)._1
      println(s"Recommended optimal buffer size: ${optimalSize}MB")
      logger.info(s"Recommended optimal buffer size: ${optimalSize}MB")
      
    } catch {
      case e: Exception =>
        logger.error(s"Error in benchmark: ${e.getMessage}", e)
        println(s"Error in benchmark: ${e.getMessage}")
        e.printStackTrace()
    } finally {
      spark.stop()
    }
  }
  
  /**
   * Benchmark MD5 calculation with different buffer sizes
   *
   * @param hdfsPath Path to the HDFS file
   * @param spark SparkSession
   * @param bufferSizesToTest Array of buffer sizes to test in MB
   * @return Map with results - method name to (hash, duration) pairs
   */
  def benchmarkMD5(hdfsPath: String, spark: SparkSession, bufferSizesToTest: Array[Int]): Map[Any, (String, Double)] = {
    logger.info(s"Starting MD5 benchmark for file: $hdfsPath")
    
    val results = scala.collection.mutable.Map[Any, (String, Double)]()
    val fs = FileSystem.get(spark.sparkContext.hadoopConfiguration)
    val fileSize = fs.getFileStatus(new Path(hdfsPath)).getLen
    
    logger.info(s"File size: $fileSize bytes (${fileSize / (1024 * 1024 * 1024.0)} GB)")
    
    // Test Google Guava implementation
    try {
      logger.info("Testing Google Guava Hashing for HDFS")
      
      val startTimeGuava = System.nanoTime()
      val md5HashGuava = calculateMD5WithGuavaHDFS(hdfsPath, fs)
      val endTimeGuava = System.nanoTime()
      val durationGuava = (endTimeGuava - startTimeGuava) / 1e9
      
      val throughputGuava = (fileSize / 1024.0 / 1024.0) / durationGuava
      
      logger.info(s"Guava MD5 Hash: $md5HashGuava")
      logger.info(f"Guava Time: $durationGuava%.3f seconds")
      logger.info(f"Guava Throughput: $throughputGuava%.2f MB/second")
      
      results("Guava") = (md5HashGuava, durationGuava)
    } catch {
      case e: Exception =>
        logger.error(s"Error with Guava method: ${e.getMessage}", e)
    }
    
    // Test DigestUtils implementation (for smaller files)
    // Commented out but can be uncommented for files < 2GB
    /*
    if (fileSize < 2L * 1024 * 1024 * 1024) { // Only for files < 2GB
      try {
        logger.info("Testing Apache Commons DigestUtils.md5Hex method")
        
        val startTime = System.nanoTime()
        val md5Checksum = org.apache.commons.codec.digest.DigestUtils.md5Hex(fs.open(new Path(hdfsPath)))
        val endTimeDigestUtils = System.nanoTime()
        val durationDigestUtils = (endTimeDigestUtils - startTime) / 1e9
        
        val throughputMBps = (fileSize / 1024.0 / 1024.0) / durationDigestUtils
        
        logger.info(s"DigestUtils MD5 checksum: $md5Checksum")
        logger.info(f"DigestUtils time taken: $durationDigestUtils%.3f seconds")
        logger.info(f"DigestUtils throughput: $throughputMBps%.2f MB/second")
        
        results("DigestUtils") = (md5Checksum, durationDigestUtils)
      } catch {
        case e: Exception =>
          logger.error(s"Error with DigestUtils method: ${e.getMessage}", e)
      }
    } else {
      logger.info("Skipping DigestUtils method for large file (>2GB)")
    }
    */
    
    // Test multi-threaded parallel I/O approach
    try {
      logger.info("Testing multi-threaded parallel I/O approach with 8 threads")
      
      val startTime = System.nanoTime()
      val md5Hash = computeMD5WithParallelIO(hdfsPath, spark, 8)
      val endTime = System.nanoTime()
      val duration = (endTime - startTime) / 1e9
      
      val throughput = (fileSize / 1024.0 / 1024.0) / duration
      
      logger.info(s"Parallel I/O MD5 Hash: $md5Hash")
      logger.info(f"Parallel I/O Time: $duration%.3f seconds")
      logger.info(f"Parallel I/O Throughput: $throughput%.2f MB/second")
      
      results("ParallelIO") = (md5Hash, duration)
    } catch {
      case e: Exception =>
        logger.error(s"Error with Parallel I/O method: ${e.getMessage}", e)
    }
    
    // Test custom chunked implementation with different buffer sizes
    for (bufferSizeMB <- bufferSizesToTest) {
      try {
        logger.info(s"---------- Testing ${bufferSizeMB}MB buffer size ----------")
        
        // Run with best of 1 try for simplicity
        val (hash, time) = computeMD5(hdfsPath, bufferSizeMB, spark)
        
        results(bufferSizeMB) = (hash, time)
        logger.info(f"Buffer size: ${bufferSizeMB}MB, Time: $time%.3f seconds")
      } catch {
        case e: Exception =>
          logger.error(s"Error testing ${bufferSizeMB}MB buffer: ${e.getMessage}", e)
      }
    }
    
    // Print summary
    logger.info("===== MD5 Benchmark Results =====")
    
    // Sort by speed (fastest first)
    val sortedResults = results.toSeq.sortBy(_._2._2)
    
    for ((method, (hash, time)) <- sortedResults) {
      val throughput = (fileSize / 1024.0 / 1024.0) / time
      logger.info(f"Method: $method, Time: $time%.3f seconds, Throughput: $throughput%.2f MB/s")
      println(f"Method: $method, Time: $time%.3f seconds, Throughput: $throughput%.2f MB/s")
    }
    
    // Check if all hashes match
    val hashes = results.values.map(_._1).toSet
    if (hashes.size == 1) {
      logger.info("All MD5 hashes match across methods ✓")
      println("All MD5 hashes match across methods ✓")
    } else {
      logger.error("WARNING: Not all MD5 hashes match! Verification failed!")
      println("WARNING: Not all MD5 hashes match! Verification failed!")
    }
    
    results.toMap
  }
  
  /**
   * Compute MD5 hash using chunked streaming approach
   * 
   * This approach reads the file in chunks of specified size and
   * updates the MD5 hash incrementally.
   *
   * @param hdfsPath Path to the HDFS file
   * @param bufferSizeMB Buffer size in megabytes
   * @param spark SparkSession
   * @return Tuple of (MD5 hash string, duration in seconds)
   */
  def computeMD5(hdfsPath: String, bufferSizeMB: Int, spark: SparkSession): (String, Double) = {
    val bufferSize = bufferSizeMB * 1024 * 1024
    
    logger.info(s"Starting MD5 calculation with ${bufferSizeMB}MB buffer size for file: $hdfsPath")
    
    val startTime = System.nanoTime()
    
    val fs = FileSystem.get(spark.sparkContext.hadoopConfiguration)
    val rawInputStream = fs.open(new Path(hdfsPath))
    val inputStream = new BufferedInputStream(rawInputStream, bufferSize)
    
    val fileSize = fs.getFileStatus(new Path(hdfsPath)).getLen
    val totalMB = fileSize / (1024 * 1024) // Add this line to define totalMB
    val md = MessageDigest.getInstance("MD5")
    val buffer = new Array[Byte](bufferSize)
    
    var bytesRead = 0
    var totalBytesRead = 0L
    var lastReportTime = System.nanoTime()
    val reportIntervalMs = 10000 // Report every 10 seconds
    
    try {
      while ({ bytesRead = inputStream.read(buffer); bytesRead != -1 }) {
        md.update(buffer, 0, bytesRead)
        totalBytesRead += bytesRead
        
        // Progress reporting
        val currentTime = System.nanoTime()
        if ((currentTime - lastReportTime) / 1000000 > reportIntervalMs) {
          val percent = (totalBytesRead.toDouble / fileSize) * 100
          val elapsedSec = (currentTime - startTime) / 1e9
          val mbProcessed = totalBytesRead / (1024 * 1024)
          val mbPerSec = mbProcessed / elapsedSec
          
          logger.info(f"Progress: $percent%.2f%% ($mbProcessed/$totalMB MB), Speed: $mbPerSec%.2f MB/s")
          
          lastReportTime = currentTime
        }
      }
    } finally {
      try {
        inputStream.close()
      } catch {
        case e: Exception => logger.warn(s"Error closing input stream: ${e.getMessage}")
      }
    }
    
    val md5Hash = md.digest().map("%02x".format(_)).mkString
    val endTime = System.nanoTime()
    val duration = (endTime - startTime) / 1e9 // Convert to seconds
    
    logger.info(f"MD5 hash: $md5Hash (buffer size: ${bufferSizeMB}MB, time: $duration%.3f seconds)")
    
    (md5Hash, duration)
  }
  
  /**
   * Calculate MD5 using Google Guava (for HDFS files)
   * 
   * @param hdfsFilePath Path to the file in HDFS
   * @param fs FileSystem instance
   * @return MD5 hash string
   */
  def calculateMD5WithGuavaHDFS(hdfsFilePath: String, fs: FileSystem): String = {
    val path = new Path(hdfsFilePath)
    
    // Create a ByteSource from HDFS InputStream
    val byteSource = new ByteSource() {
      override def openStream(): java.io.InputStream = {
        fs.open(path)
      }
    }
    
    byteSource.hash(Hashing.md5()).toString
  }
  
  /**
   * Calculate MD5 hash using multithreaded I/O approach
   * 
   * This approach uses multiple threads to read chunks in parallel
   * while maintaining a single MD5 hash that's updated in the correct order.
   * Excellent for I/O bound workloads on fast storage systems.
   * 
   * @param hdfsPath Path to HDFS file
   * @param spark SparkSession (needed for HDFS configuration)
   * @param ioThreads Number of parallel reader threads (default: 4)
   * @return MD5 hash string
   */
  def computeMD5WithParallelIO(hdfsPath: String, spark: SparkSession, ioThreads: Int = 4): String = {
    val startTime = System.nanoTime()
    logger.info(s"Starting multithreaded MD5 calculation for file: $hdfsPath with $ioThreads reader threads")
    
    // Get file info from HDFS
    val fs = FileSystem.get(spark.sparkContext.hadoopConfiguration)
    val path = new Path(hdfsPath)
    val fileSize = fs.getFileStatus(path).getLen
    val fileSizeGB = fileSize / (1024.0 * 1024 * 1024)
    
    logger.info(f"File size: $fileSize bytes ($fileSizeGB%.2f GB)")
    
    // Set up the MD5 calculator
    val md = MessageDigest.getInstance("MD5")
    
    // Create our thread pool for parallel reads
    val executor = Executors.newFixedThreadPool(ioThreads)
    implicit val ec = ExecutionContext.fromExecutor(executor)
    
    // This queue holds chunks that have been read but not yet processed
    // We'll use it to keep chunks in order while allowing parallel reads
    val blockingQueue = new LinkedBlockingQueue[ChunkData](ioThreads * 2) // Buffer for smoother processing
    
    // Chunk size - 16MB is often a good balance for HDFS
    val chunkSize = 16 * 1024 * 1024
    val numChunks = Math.ceil(fileSize.toDouble / chunkSize).toLong
    
    logger.info(s"Using chunk size: ${chunkSize / (1024 * 1024)} MB")
    logger.info(s"Total chunks to process: $numChunks")
    
    try {
      // This consumer thread takes chunks from the queue and updates the hash
      // It maintains order by tracking which chunk is next to process
      val consumer = Future {
        var processedChunks = 0L
        var nextChunkToProcess = 0L
        val pendingChunks = scala.collection.mutable.Map[Long, ChunkData]()
        
        while (processedChunks < numChunks) {
          // Wait for the next chunk to arrive
          val chunk = blockingQueue.take() // Blocks until a chunk is available
          
          if (chunk.index == nextChunkToProcess) {
            // This is the next chunk we need - process it immediately
            md.update(chunk.data, 0, chunk.size)
            processedChunks += 1
            nextChunkToProcess += 1
            
            // Check if we already have the next chunks waiting in our pending map
            while (pendingChunks.contains(nextChunkToProcess)) {
              val pendingChunk = pendingChunks(nextChunkToProcess)
              md.update(pendingChunk.data, 0, pendingChunk.size)
              pendingChunks.remove(nextChunkToProcess)
              processedChunks += 1
              nextChunkToProcess += 1
            }
          } else {
            // This chunk is out of order - store it until its turn
            pendingChunks(chunk.index) = chunk
          }
          
          // Log progress periodically
          if (processedChunks % 100 == 0) {
            logger.info(f"Processed $processedChunks/$numChunks chunks (${processedChunks * 100.0 / numChunks}%.2f%%)")
          }
        }
        
        logger.info("All chunks processed")
      }
      
      // Create reader futures to fill the queue with chunks in parallel
      val readers = (0L until numChunks).map { chunkIndex =>
        Future {
          val offset = chunkIndex * chunkSize
          val length = Math.min(chunkSize, fileSize - offset).toInt
          val buffer = new Array[Byte](length)
          
          // Open the file and read this chunk
          val is = fs.open(path)
          try {
            is.seek(offset)
            var bytesRead = 0
            var totalRead = 0
            
            // Read the whole chunk, handling partial reads
            while (totalRead < length) {
              bytesRead = is.read(buffer, totalRead, length - totalRead)
              if (bytesRead == -1) break
              totalRead += bytesRead
            }
            
            // Add this chunk to the queue for the consumer thread
            val chunk = ChunkData(chunkIndex, buffer, totalRead)
            blockingQueue.put(chunk) // Will block if queue is full
            
            // Log progress occasionally
            if (chunkIndex % 100 == 0) {
              logger.info(f"Read chunk $chunkIndex/$numChunks (${chunkIndex * 100.0 / numChunks}%.2f%%)")
            }
          } finally {
            is.close()
          }
        }
      }
      
      // Wait for all reader threads to finish
      Await.result(Future.sequence(readers), Duration.Inf)
      
      // Wait for the consumer to process all chunks
      Await.result(consumer, Duration.Inf)
      
      // Get the final hash
      val md5Hash = md.digest().map("%02x".format(_)).mkString
      
      val endTime = System.nanoTime()
      val totalTimeSec = (endTime - startTime) / 1e9
      
      logger.info(s"Multithreaded MD5 calculation completed: $md5Hash")
      logger.info(f"Total processing time: $totalTimeSec%.2f seconds")
      logger.info(f"Throughput: ${(fileSize / 1024.0 / 1024.0) / totalTimeSec}%.2f MB/second")
      
      md5Hash
    } finally {
      // Clean up our thread pool
      executor.shutdown()
    }
  }
}
