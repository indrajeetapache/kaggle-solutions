import org.apache.spark.sql.{DataFrame, SparkSession}
import org.apache.spark.sql.functions._
import org.apache.spark.sql.expressions.Window

val spark = SparkSession.builder()
  .appName("example")
  .master("local[*]")  // Adjust based on your setup
  .getOrCreate()

import spark.implicits._

// Sample DataFrame
val df = Seq(
  ("A"),
  ("A"),
  ("B"),
  ("B"),
  ("B"),
  ("C"),
  ("C"),
  ("D"),
  ("D")
).toDF("key")

// Function to perform recursive aggregation
def recursiveAggregation(df: DataFrame, callCount: Int = 1): DataFrame = {
  // Log the call count
  println(s"Recursive call count: $callCount")

  // Define a window specification with an orderBy clause on the key column
  val windowSpec = Window.orderBy("key")

  // Generate a row number for each row
  val dfWithId = df.withColumn("value", row_number().over(windowSpec))

  // Get the maximum value in the value column, which indicates the total count
  val maxId = dfWithId.agg(max("value")).head.getLong(0)

  // If maxId is 1, it means there's only one row, so return the DataFrame
  if (maxId == 1) {
    return dfWithId
  }

  // Calculate the range dynamically
  val range = maxId match {
    case count if count < 50 => 2
    case count => (count / 10).toInt
  }

  // Assign a constant partition_id if maxId is less than 50, otherwise calculate partition_id based on the range
  val dfWithPartitionId = if (maxId < 50) {
    dfWithId.withColumn("partition_id", lit(0))
  } else {
    dfWithId.withColumn("partition_id", floor(col("value") / range))
// val randomExpression = expr(s"floor($n + (rand() * (${range} - $n)))")  // generates a random number between n and range
  dfWithId.withColumn("partition_id", randomExpression)
  }

  // Group by the partition ID and key, then aggregate
  val aggregatedDF = dfWithPartitionId.groupBy("partition_id", "key")
    .agg(md5(concat_ws("", collect_list(dfWithPartitionId("value").cast("string")))).alias("hash"))

  // Call the function recursively with the new DataFrame, incrementing the call count
  recursiveAggregation(aggregatedDF.drop("partition_id", "value"), callCount + 1)
}

// Call the recursive aggregation function
val resultDF = recursiveAggregation(df)
resultDF.show()


val resultDF = df.withColumn("substring", substring(col("yourColumn"), 1, 5))



=======


val resultDf = rowCount match {
      case count if count > 10 =>
        // Perform operations for count > 10
        df.filter(df("some_column") > some_value)
      
      case count if count > 5 && count <= 10 =>
        // Perform operations for 5 < count <= 10
        df.withColumn("new_column", df("some_column") * some_value)
      
      case _ =>
        // Optionally handle other cases
        df
    }
