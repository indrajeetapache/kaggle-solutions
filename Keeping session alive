from pyspark.sql.functions import *

# For ApproxQuantiles specifically - extract key-value pairs
df_quantiles = df.filter(col("metric_name") == "ApproxQuantiles") \
    .withColumn("quantile", regexp_extract(col("metric_values"), r"ApproxQuantiles-([\d.]+)", 1).cast("double")) \
    .withColumn("value", regexp_extract(col("metric_values"), r"ApproxQuantiles-[\d.]+,\s*([\dE.-]+)", 1).cast("double")) \
    .filter(col("quantile").isNotNull() & col("value").isNotNull())

# For other metrics - extract single values
df_other = df.filter(col("metric_name") != "ApproxQuantiles") \
    .withColumn("value", regexp_extract(col("metric_values"), r"([\d.E-]+)", 1).cast("double"))

# Union and pivot
df_combined = df_quantiles.select("dataset_name", "col_name", "metric_name", "value") \
    .union(df_other.select("dataset_name", "col_name", "metric_name", "value"))

final_df = df_combined.groupBy("col_name", "dataset_name") \
    .pivot("metric_name") \
    .agg(first("value"))
