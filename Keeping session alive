
import org.apache.spark.sql.SparkSession

def getPartitionColumns(spark: SparkSession, tableName: String): Seq[String] = {
  // Execute 'SHOW CREATE TABLE' command to get the table's DDL
  val createTableDDL = spark.sql(s"SHOW CREATE TABLE $tableName").collect().map(_.getString(0)).mkString("\n")

  // Extract the partition column details from the DDL
  val partitionPattern = "(?i)PARTITIONED BY\\s*\\(([^)]+)\\)".r
  partitionPattern.findFirstMatchIn(createTableDDL) match {
    case Some(matched) => 
      val partitionPart = matched.group(1)
      partitionPart.split(",").map(_.trim.split("\\s+")(0)) // Extracting column names
    case None => 
      Array[String]() // Return an empty array if the table is not partitioned
  }
}

// Usage
val spark: SparkSession = // your SparkSession
val tableName = "your_table_name"
val partitionColumns = getPartitionColumns(spark, tableName)
partitionColumns.foreach(println)


===================================================

import org.apache.spark.sql.DataFrame
import org.apache.spark.sql.functions.col
import scala.util.matching.Regex
import java.time.{LocalDate, Instant, ZoneId}
import java.time.format.DateTimeFormatter

def isString(value: String): Boolean = {
    val stringPattern: Regex = "^[A-Za-z]+$".r
    stringPattern.findFirstIn(value).isDefined
}

def isLikelyDate(value: String): Boolean = {
    // Get the current date
    val currentDate = LocalDate.now()
    val currentYear = currentDate.getYear.toString
    val currentYearMonth = currentDate.format(DateTimeFormatter.ofPattern("yyyyMM"))
    val currentDateStr = currentDate.format(DateTimeFormatter.ofPattern("yyyyMMdd"))

    // Check for Unix timestamp pattern
    val isUnixTimestamp = value.matches("\\d{10}")

    val dateValue = if (isUnixTimestamp) {
        // Convert Unix timestamp to LocalDate
        val instant = Instant.ofEpochSecond(value.toLong)
        val localDate = instant.atZone(ZoneId.systemDefault()).toLocalDate
        localDate.format(DateTimeFormatter.ofPattern("yyyyMMdd"))
    } else {
        value
    }

    // Define regex patterns for various date formats and the current date string
    val datePatterns = Map(
      "\\d{8}" -> currentDateStr, // yyyymmdd
      "\\d{6}" -> currentYearMonth, // yyyymm
      "\\d{4}" -> currentYear // yyyy
    )

    // Check if the value matches any of the date patterns and corresponds to the current date
    datePatterns.exists { case (pattern, dateString) =>
        dateValue.matches(pattern) && dateValue == dateString
    }
}

def validatePartitionColumns(df: DataFrame, partitionColumns: Array[String]): Array[String] = {
    partitionColumns.filter { colName =>
        // Cast the column to string and get the first value, filtering out non-digit characters
        val sampleValue = df.select(col(colName).cast("string")).first().getString(0).filter(_.isDigit)

        // Determine if the value is likely a date or a string
        !isLikelyDate(sampleValue) && !isString(sampleValue)
    }
}

// Usage example
val df1: DataFrame = // Your DataFrame
val partitionColumns = Array("example_date_column", "example_string_column") // Replace with actual column names
val validatedColumns = validatePartitionColumns(df1, partitionColumns)

// Print validated columns
validatedColumns.foreach(println)


===========



import java.time.{LocalDate, Instant, ZoneId}
import java.time.format.{DateTimeFormatter, DateTimeParseException}

def isUnixTimestamp(value: String): Boolean = {
    value.matches("\\d{10}")
}

def convertUnixTimestampToDate(value: String): String = {
    val instant = Instant.ofEpochSecond(value.toLong)
    val localDate = instant.atZone(ZoneId.systemDefault()).toLocalDate
    localDate.format(DateTimeFormatter.ofPattern("yyyyMMdd"))
}

def isValidDate(value: String, format: String): Boolean = {
    try {
        LocalDate.parse(value, DateTimeFormatter.ofPattern(format))
        true
    } catch {
        case _: DateTimeParseException => false
    }
}

def isLikelyDate(value: String): Boolean = {
    val sanitizedValue = if (isUnixTimestamp(value)) {
        convertUnixTimestampToDate(value)
    } else {
        value.filter(_.isDigit)
    }

    val dateFormats = List(
        "yyyyMMdd",
        "yyyy-MM-dd",
        "yyyy/MM/dd",
        "yyyy",
        "yyyyMM",
        "yyyy-MM",
        "yyyy/MM"
    )

    dateFormats.exists(format => {
        if (format.length <= 6) // Formats without day (yyyy or yyyymm)
            sanitizedValue == LocalDate.now().format(DateTimeFormatter.ofPattern(format))
        else
            isValidDate(sanitizedValue, format) && LocalDate.parse(sanitizedValue, DateTimeFormatter.ofPattern(format)).isEqual(LocalDate.now())
    })
}

// Example usage
println(isLikelyDate("2023"))       // True if the current year is 2023
println(isLikelyDate("202312"))     // True if the current year-month is December 2023
println(isLikelyDate("2023-12"))    // True if the current year-month is December 2023
println(isLikelyDate("2023/12"))    // True if the current year-month is December 2023
println(isLikelyDate("1702900390")) // True if the Unix timestamp corresponds to the current date
// Example usage
println(isLikelyDate("2023"))       // True if the current year is 2023
println(isLikelyDate("202312"))     // True if the current year-month is December 2023
println(isLikelyDate("2023-12"))    // True if the current year-month is December 2023
println(isLikelyDate("2023/12"))    // True if the current year-month is December 2023


=========================

import org.apache.spark.sql.DataFrame
import org.apache.spark.sql.functions._

val partitionColumns = Array("col1", "col2", "col3") // Example partition columns
val source_df: DataFrame = // Your source DataFrame

// Form the SELECT part of the query
val selectStatement = partitionColumns.mkString("SELECT ", ", ", " FROM yourTableName")

// Assuming we want to select unique values for the WHERE clause from the first row
val uniqueRow = source_df.select(partitionColumns.map(col): _*).distinct().first()

// Form the WHERE part of the query
val whereConditions = partitionColumns.map(colName => s"$colName = '${uniqueRow.getAs[String](colName)}'").mkString(" WHERE ", " AND ", "")

// Combine to form the complete SQL query
val sqlQuery = selectStatement + whereConditions

println(sqlQuery)

=====================================================================
import org.apache.spark.sql.DataFrame
import org.apache.spark.sql.functions.col

// Assuming source_df is your DataFrame and partitionColumns are the columns you're interested in
val partitionColumns = Array("col1", "col2", "col3") // Example partition columns
val source_df: DataFrame = // Your source DataFrame

// Step 1: Retrieve Distinct Combinations of Column Values
val distinctCombinations = source_df.select(partitionColumns.map(col): _*).distinct()

// Step 2: Retrieve Distinct Values for Each Column Separately
def getDistinctValues(df: DataFrame, columnName: String): Array[Any] = {
    df.select(columnName).distinct().collect().map(_.get(0))
}

val distinctValuesPerColumn = partitionColumns.map(columnName => {
    val distinctValues = getDistinctValues(source_df, columnName)
    (columnName, distinctValues)
}).toMap

// Example: Printing Results
println("Distinct Combinations of Column Values:")
distinctCombinations.show()

println("Distinct Values Per Column:")
distinctValuesPerColumn.foreach { case (colName, values) =>
    println(s"$colName: ${values.mkString(", ")}")
}

========

import org.apache.spark.sql.{DataFrame, Row}
import org.apache.spark.sql.functions.col

val partitionColumns = Array("col1", "col2", "col3") // Replace with your actual column names
val source_df: DataFrame = // Your source DataFrame
val databaseName = "yourDatabase" // Replace with your actual database name
val tableName = "yourTable"       // Replace with your actual table name

def getDistinctValuesForInClause(df: DataFrame, columnName: String): String = {
    val distinctValues = df.select(col(columnName)).distinct().collect()
    if (distinctValues.isEmpty) ""
    else distinctValues.map(row => s"'${row.getAs[String](columnName)}'").mkString(", ")
}

val whereConditions = partitionColumns.flatMap(colName => {
    val distinctValues = getDistinctValuesForInClause(source_df, colName)
    if (distinctValues.nonEmpty) Some(s"$colName IN ($distinctValues)") else None
}).mkString(" WHERE ", " AND ", "")

val completeSqlQuery = if (whereConditions.trim.isEmpty) {
    s"SELECT ${partitionColumns.mkString(", ")} FROM $databaseName.$tableName"
} else {
    s"SELECT ${partitionColumns.mkString(", ")} FROM $databaseName.$tableName$whereConditions"
}

println(completeSqlQuery)

=======
// List of column names to check, converted to lower case and trimmed
val columnsToCheck = List("audit_file_name", "audit_run_id").map(_.trim.toLowerCase)

// Function to check if all columns in the list are present in the DataFrame (case-insensitive and whitespace-insensitive)
def areAllColumnsPresentCaseAndWhitespaceInsensitive(df: DataFrame, columns: List[String]): Boolean = {
  val formattedDfColumns = df.columns.map(_.trim.toLowerCase).toSet
  columns.forall(formattedDfColumns.contains)
}

// Validate the presence of all columns (case-insensitive and whitespace-insensitive)
val areAllPresent = areAllColumnsPresentCaseAndWhitespaceInsensitive(source_df, columnsToCheck)
