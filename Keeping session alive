val dbtable = "(select * from PLNAPP1.MATCHED_APPLICATIONS where APPLICATION_RECV_DT > TO_TIMESTAMP('09-MAY-22 10:39:51', 'DD-MON-YY HH24:MI:SS')) query"
val dbtable = "(select * from PLNAPP1.MATCHED_APPLICATIONS where APPLICATION_RECV_DT > TO_TIMESTAMP('09-MAY-22 10:39:51 AM', 'DD-MON-YY HH:MI:SS AM')) query"
val dbtable = "(select * from PLNAPP1.MATCHED_APPLICATIONS where APPLICATION_RECV_DT > TO_TIMESTAMP('09-MAY-22 10:39:51', 'DD-MON-YY HH24:MI:SS')) query"

val partitionColumn = "your_date_column"
val lowerBound = "TO_DATE('01-JAN-20 12:00:00.00 AM', 'DD-MON-YY HH12:MI:SS.FF AM')"
val upperBound = "TO_DATE('31-DEC-20 11:59:59.99 PM', 'DD-MON-YY HH12:MI:SS.FF AM')"
val numPartitions = 10


// Define your query with a numeric representation of the date
val query = """
  (SELECT 
    *,
    TO_NUMBER(TO_CHAR(your_date_column, 'YYYYMMDDHHMI')) as date_num 
   FROM your_table) tmp_table
"""

val partitionColumn = "date_num"
val lowerBound = "202001010000" // Represents 01-JAN-20 00:00
val upperBound = "202012312359" // Represents 31-DEC-20 23:59
val numPartitions = 10

// Load data in parallel
val df = spark.read.jdbc(
  url = "jdbc:oracle:thin:@//your_host:port/your_service",
  table = query,
  columnName = partitionColumn,
  lowerBound = lowerBound,
  upperBound = upperBound,
  numPartitions = numPartitions,
  connectionProperties = connectionProperties
)
spark.conf.set("spark.sql.legacy.timeParserPolicy", "LEGACY")

.option("lowerBound", "TO_DATE('01-JAN-20 00:00:00', 'DD-MON-YY HH24:MI:SS')")
.option("upperBound", "TO_DATE('31-DEC-20 23:59:59', 'DD-MON-YY HH24:MI:SS')")


val query = """
  (SELECT 
    *,
    TO_NUMBER(TO_CHAR(your_date_column, 'YYYYMMDDHH24MISS')) as date_num 
   FROM your_table) tmp_table
"""

// Then use date_num as your partitionColumn
.option("partitionColumn", "date_num")
.option("lowerBound", "20200101000000")
.option("upperBound", "20201231235959")

.option("lowerBound", "TO_DATE('01-JAN-20 00:00:00', 'DD-MON-YY HH24:MI:SS')")
.option("upperBound", "TO_DATE('31-DEC-20 23:59:59', 'DD-MON-YY HH24:MI:SS')")

========

val query = """
  (SELECT 
    *,
    (CAST(your_date_column AS DATE) - DATE '1970-01-01') * 86400 as unix_timestamp 
   FROM your_table) tmp_table
"""

val df = spark.read.jdbc(
  url = "jdbc:oracle:thin:@//your_host:port/your_service",
  table = query,
  columnName = "unix_timestamp",
  lowerBound = "1577836800", // Unix timestamp for 01-JAN-20 00:00:00
  upperBound = "1609459199", // Unix timestamp for 31-DEC-20 23:59:59
  numPartitions = 10,
  connectionProperties = connectionProperties
)


val query = """
  (SELECT 
    *,
    (CAST(your_date_column AS TIMESTAMP) - TIMESTAMP '1970-01-01 00:00:00') * 86400 as unix_timestamp 
   FROM your_table) tmp_table
"""
SELECT * 
FROM PLNAPP1.MATCHED_APPLICATIONS 
WHERE APPLICATION_RECV_DT >= TO_TIMESTAMP('2022-05-09 10:39:51', 'YYYY-MM-DD HH24:MI:SS')
  AND APPLICATION_RECV_DT <= TO_TIMESTAMP('2022-05-16 00:06:17', 'YYYY-MM-DD HH24:MI:SS');
====================

val jdbcURL = "jdbc:oracle:thin:@oraasgt71-scan.nam.nsroot.net:8899/HAOTAPP25"
val driver = "oracle.jdbc.driver.OracleDriver"
val partitionColumn = "APPLICATION_RECV_DT"

// Define string timestamp values for partitioning in Spark (these will not include TO_TIMESTAMP)
val lowerBound = "2022-05-09 10:39:51"
val upperBound = "2022-05-16 00:06:17"

// Specify the number of partitions
val numPartitions = 2

// Define the SQL query, using TO_TIMESTAMP within the query itself for Oracle's side of processing
val dbtable = s"""
  (SELECT * 
   FROM PLNAPP1.MATCHED_APPLICATIONS 
   WHERE APPLICATION_RECV_DT >= TO_TIMESTAMP('$lowerBound', 'YYYY-MM-DD HH24:MI:SS')
     AND APPLICATION_RECV_DT <= TO_TIMESTAMP('$upperBound', 'YYYY-MM-DD HH24:MI:SS')
  ) query
"""

// Create a Map for the JDBC options
val oracleReadOptions = Map(
  "url" -> jdbcURL,
  "driver" -> driver,
  "dbtable" -> dbtable,        // Query passed here ensures filtering with TO_TIMESTAMP
  "user" -> "EAPCDERPED",
  "password" -> "mC9y3Ult",
  "partitionColumn" -> partitionColumn,  // Spark will partition based on this
  "lowerBound" -> lowerBound,  // Raw string value for partitioning
  "upperBound" -> upperBound,  // Raw string value for partitioning
  "numPartitions" -> numPartitions
)

// Load data from Oracle using Spark JDBC
val df = spark.read.format("jdbc").options(oracleReadOptions).load()

df.show()  // Show the result to verify correctness
