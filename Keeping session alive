import org.apache.spark.sql.functions._
import java.util.zip.{GZIPOutputStream, DeflaterOutputStream}
import java.io.ByteArrayOutputStream
import org.xerial.snappy.Snappy
import com.github.luben.zstd.Zstd
import org.apache.spark.sql.types.BinaryType

// Define a function to apply the desired compression
def compressColumnData(input: String, method: String): Array[Byte] = {
  if (input == null) {
    null
  } else {
    val bos = new ByteArrayOutputStream()
    method.toLowerCase match {
      case "gzip" =>
        val gzip = new GZIPOutputStream(bos)
        gzip.write(input.getBytes("UTF-8"))
        gzip.close()
      case "zlib" =>
        val zlib = new DeflaterOutputStream(bos)
        zlib.write(input.getBytes("UTF-8"))
        zlib.close()
      case "snappy" =>
        return Snappy.compress(input.getBytes("UTF-8"))
      case "zstd" =>
        return Zstd.compress(input.getBytes("UTF-8"))  // ZSTD compression
      case _ =>
        throw new IllegalArgumentException(s"Unsupported compression method: $method")
    }
    bos.toByteArray
  }
}

// Define a UDF to apply this function to a column
def createCompressionUDF(method: String) = udf((input: String) => compressColumnData(input, method), BinaryType)


========


import org.apache.spark.sql.functions._

// Step 1: Add a unique ID to each row in the original DataFrame
val dfWithId = df.withColumn("row_id", monotonically_increasing_id())

// Step 2: Select the specific column and row_id, and save it with compression
val columnToCompress = "your_column_name" // specify the column to compress

val compressedColumnDf = dfWithId.select("row_id", columnToCompress)

// Save this compressed column DataFrame with file compression
compressedColumnDf.write
  .mode("overwrite")
  .option("compression", "gzip")  // specify your compression method here
  .parquet("/path/to/compressed_column")

// Step 3: Read back the compressed column DataFrame
val compressedColumnDfRead = spark.read
  .parquet("/path/to/compressed_column")

// Step 4: Join the compressed column back with the original DataFrame
val finalDf = dfWithId.join(compressedColumnDfRead, "row_id")

// Drop row_id if itâ€™s not needed anymore
val finalDfWithoutRowId = finalDf.drop("row_id")

// Step 5: Write the final DataFrame to Snowflake
finalDfWithoutRowId.write
  .format("snowflake")
  .options(snowflakeOptions)  // Add your Snowflake connection options here
  .option("dbtable", "your_snowflake_table")
  .save()

==============


import cats.effect.{ContextShift, IO, Resource, Sync}
import fs2.Stream
import fs2.io.readInputStream
import org.apache.hadoop.conf.Configuration
import org.apache.hadoop.fs.{FileSystem, Path, FSDataInputStream}
import scala.concurrent.ExecutionContext

// Define your ContextShift and Sync instances
implicit val cs: ContextShift[IO] = IO.contextShift(ExecutionContext.global)
implicit val syncIO: Sync[IO] = IO.ioConcurrentEffect

def readHDFSFileLineByLine(hdfsPath: String): IO[String] = {
  val configuration = new Configuration()
  val fs = FileSystem.get(configuration)

  // Resource for HDFS InputStream with logging
  def inputStreamResource: Resource[IO, java.io.InputStream] = Resource.make {
    IO {
      println(s"Opening HDFS file at path: $hdfsPath")
      fs.open(new Path(hdfsPath))  // fs.open returns FSDataInputStream
    }.widen[InputStream]  // Widen to InputStream to match expected type
  } { inputStream =>
    IO {
      println(s"Closing HDFS file at path: $hdfsPath")
      inputStream.close()
    }.handleErrorWith { ex =>
      IO(println(s"Error closing HDFS file at path: $hdfsPath: $ex"))
    }
  }

  inputStreamResource.use { inputStream =>
    readInputStream(IO.pure(inputStream), chunkSize = 8192)
      .through(fs2.text.utf8Decode)
      .through(fs2.text.lines)
      .intersperse("\n")
      .compile
      .string
      .flatTap(content => IO(println(s"Finished reading HDFS file at path: $hdfsPath, content size: ${content.length}")))
  }
}
=============


import cats.effect.{Blocker, ContextShift, IO, Resource, Sync}
import fs2.Stream
import fs2.io.readInputStream
import org.apache.hadoop.conf.Configuration
import org.apache.hadoop.fs.{FileSystem, Path, FSDataInputStream}
import scala.concurrent.ExecutionContext

// Define your ContextShift and Blocker
implicit val cs: ContextShift[IO] = IO.contextShift(ExecutionContext.global)
val blocker: Blocker = Blocker.liftExecutionContext(ExecutionContext.global)

def readHDFSFileLineByLine(hdfsPath: String): IO[String] = {
  val configuration = new Configuration()
  val fs = FileSystem.get(configuration)

  // Resource for HDFS InputStream with logging
  def inputStreamResource: Resource[IO, java.io.InputStream] = Resource.make {
    IO {
      println(s"Opening HDFS file at path: $hdfsPath")
      fs.open(new Path(hdfsPath))
    }
  } { inputStream =>
    IO {
      println(s"Closing HDFS file at path: $hdfsPath")
      inputStream.close()
    }.handleErrorWith { ex =>
      IO(println(s"Error closing HDFS file at path: $hdfsPath: $ex"))
    }
  }

  inputStreamResource.use { inputStream =>
    readInputStream(
      IO.pure(inputStream),
      chunkSize = 8192,
      blocker = blocker,       // Provide the blocker
      closeAfterUse = true     // Set to true or false as needed
    )
    .through(fs2.text.utf8Decode)
    .through(fs2.text.lines)
    .intersperse("\n")
    .compile
    .string
    .flatTap(content => IO(println(s"Finished reading HDFS file at path: $hdfsPath, content size: ${content.length}")))
  }
}

import cats.effect.{Blocker, ContextShift, IO, Sync}
import scala.concurrent.ExecutionContext

implicit val cs: ContextShift[IO] = IO.contextShift(ExecutionContext.global)
implicit val syncIO: Sync[IO] = IO.ioConcurrentEffect
implicit val blocker: Blocker = Blocker.liftExecutionContext(ExecutionContext.global)

val filePath: String = "hdfs://path/to/your/file"
val rawStr: String = util.readHDFSFileLineByLine(filePath).unsafeRunSync()

logger.info(s"Raw string content: $rawStr")

readInputStream: This fs2 function reads data from an InputStream and converts it into a Stream[IO, Byte] (a stream of bytes).
IO.pure(inputStream): Wraps the InputStream in an IO action to make it compatible with fs2.
chunkSize = 8192: Specifies the size (in bytes) of each chunk to be read from the InputStream. Reading in chunks helps to control memory usage by not loading the entire file into memory at once.
blocker = blocker: Tells readInputStream to use the Blocker, ensuring that reading from the InputStream (a blocking operation) happens on a separate thread pool.
closeAfterUse = true: Ensures the InputStream will be closed automatically after reading, adding an extra layer of safety in resource management.
through(fs2.text.utf8Decode): This converts the byte stream into a character stream with UTF-8 encoding. It transforms the Stream[IO, Byte] into a Stream[IO, String], where each element is a UTF-8 decoded string from the bytes.

through(fs2.text.lines): This breaks the decoded text into lines. It further transforms the Stream[IO, String] into a Stream[IO, String] where each element represents a line of text in the file.
.intersperse("\n"): This inserts newline characters (\n) between each line in the stream, preserving the original line structure of the file. Without this, concatenating the lines might result in a continuous string without line breaks.
.compile.string: This operation compiles the entire Stream[IO, String] (each element representing a line) into a single String, effectively concatenating all lines in the file into one continuous string with line breaks.

===========

import org.apache.hadoop.conf.Configuration
import org.apache.hadoop.fs.{FileSystem, Path}
import scala.io.Source

def readLargeFileAsSingleString(hdfsPath: String): String = {
  val conf = new Configuration()
  val fs = FileSystem.get(conf)
  val path = new Path(hdfsPath)
  val inputStream = fs.open(path)
  
  try {
    val lines = Source.fromInputStream(inputStream).getLines()
    val content = new StringBuilder
    
    // Append each line to the StringBuilder
    lines.foreach { line =>
      content.append(line).append("\n")
    }
    
    // Return the entire content as a single string
    content.toString
  } finally {
    inputStream.close()
  }
}

// Usage
val filePath = "hdfs://path/to/your/3gb_file"
val rawStr: String = readLargeFileAsSingleString(filePath)
println(s"File content size: ${rawStr.length}")


spark-submit \
  --conf spark.sql.crossJoin.enabled=true \
  --conf spark.dynamicAllocation.minExecutors=12 \
  --conf spark.dynamicAllocation.initialExecutors=15 \
  --conf spark.dynamicAllocation.maxExecutors=20 \
  --conf spark.sql.parquet.compression.codec=snappy \
  --deploy-mode cluster \
  --driver-memory 8g \
  --executor-memory 12g \
  --conf spark.yarn.am.memoryOverhead=6g \
  --conf spark.yarn.driver.memoryOverhead=3g \
  --conf spark.yarn.executor.memoryOverhead=2g \
  --queue root.Grmas_3170_Yarn \
  path/to/your/application.jar

=======

spark-submit \
  --master yarn \
  --deploy-mode cluster \
  --name campgenrolletxt \
  --driver-memory 20g \
  --executor-memory 16g \
  --conf spark.yarn.driver.memoryOverhead=6g \
  --conf spark.yarn.executor.memoryOverhead=4g \
  --conf spark.dynamicAllocation.enabled=true \
  --conf spark.dynamicAllocation.maxExecutors=20 \
  --conf spark.hadoop.mapreduce.input.fileinputformat.split.minsize=536870912 \
  --conf spark.hadoop.mapreduce.input.fileinputformat.split.maxsize=1073741824 \
  --conf "spark.executor.extraJavaOptions=-Xmx16g" \
  --conf spark.sql.files.maxPartitionBytes=536870912 \
  --conf spark.sql.files.openCostInBytes=134217728 \
  <path-to-your-application-jar> <your-application-arguments>


val rdd = spark.sparkContext.textFile(filePath)
val entireFileAsString = rdd.reduce(_ + "\n" + _)
val entireFileAsString = coalescedRdd.aggregate("")(
  (acc, line) => if (acc.isEmpty) line else acc + "\n" + line,  // Within-partition aggregation
  (acc1, acc2) => acc1 + "\n" + acc2                             // Across-partition aggregation
)
========================================

import org.apache.spark.sql.{DataFrame, SparkSession, Row}
import org.apache.spark.sql.functions._
import org.apache.spark.sql.expressions.Window
import org.apache.spark.sql.types._
import java.nio.file.Paths

def processCsvWithMultiDelimiter(
    spark: SparkSession,
    filePath: String,
    schema: StructType,
    delimiters: Array[Char],
    replacementChar: Option[Char] = None,
    headerRowsToIgnore: Option[Int] = None,
    footerRowsToIgnore: Option[Int] = None
): DataFrame = {

  import spark.implicits._

  // Get the source file name and path
  val sourceFileName = Paths.get(filePath).getFileName.toString
  val sourceFilePath = filePath

  // Read the file as a DataFrame with a single column "line" and add unique IDs
  var rawDataFrame = spark.read
    .text(filePath)
    .withColumnRenamed("value", "line")
    .withColumn("unique_id", monotonically_increasing_id()) // Assign unique IDs

  // Remove header rows if specified
  if (headerRowsToIgnore.isDefined) {
    rawDataFrame = rawDataFrame.filter($"unique_id" > headerRowsToIgnore.get)
  }

  // Remove footer rows if specified
  if (footerRowsToIgnore.isDefined) {
    val totalRowCount = rawDataFrame.count()
    rawDataFrame = rawDataFrame.filter($"unique_id" <= (totalRowCount - footerRowsToIgnore.get))
  }

  // Step 1: Replace delimiters using either replacementChar or the first delimiter in delimiters
  val effectiveReplacementChar = replacementChar.getOrElse(delimiters.head)
  val delimiterPattern = delimiters.map(Regex.quote).mkString("|")

  val processedDataFrame = rawDataFrame.withColumn(
    "line",
    regexp_replace($"line", delimiterPattern, effectiveReplacementChar.toString)
  )

  // Step 2: Create a flag column to identify lines with unexpected field counts after replacement
  val flaggedDataFrame = processedDataFrame.withColumn(
    "flag",
    when(size(split($"line", "\\" + effectiveReplacementChar)) === schema.fields.length, 0).otherwise(1)
  )

  // Step 3: Use lead and lag with unique_id to combine lines where "flag" is 1
  val windowSpec = Window.orderBy("unique_id")

  val combinedDf = flaggedDataFrame
    .withColumn("next_line", lead("line", 1).over(windowSpec))
    .withColumn("next_flag", lead("flag", 1).over(windowSpec))
    .withColumn(
      "combined_line",
      when($"flag" === 1 && $"next_flag" === 0, concat_ws(effectiveReplacementChar.toString, $"line", $"next_line"))
        .otherwise($"line")
    )
    .withColumn("is_combined", $"flag" === 1 && $"next_flag" === 0)
    .withColumn("is_combined_part", when($"flag" === 1 && $"next_flag" === 0, lit(true)).otherwise(lit(false)))

  // Step 4: Filter out rows that are part of the next line's combination to avoid duplication
  val combinedDfWithFlag = combinedDf
  .filter(
    ($"is_combined" === true && $"next_flag" === 0) || // Keep the row where the combination is successful
    ($"is_combined" === false && $"flag" === 0)       // Keep rows that are standalone valid records
  )
  .withColumn("split_line", split($"combined_line", "\\" + effectiveReplacementChar))


  val filteredDf = combinedDfWithFlag
    .select(
      (0 until schema.fields.length).map(i => $"split_line"(i).as(schema.fields(i).name)) ++ Seq($"flag", $"line", $"is_combined"): _*
    )
    .withColumn("_corrupt_record", when($"flag" === 1 && !$"is_combined", $"line").otherwise(null))
    .withColumn("validation_flag", when($"_corrupt_record".isNotNull, lit("corrupt")).otherwise(lit("valid")))
    .drop("flag", "line", "is_combined") // Drop the flag, line, and is_combined columns if they are no longer needed
    .withColumn("source_file_name", lit(sourceFileName))
    .withColumn("source_file_path", lit(sourceFilePath))

  // Optional: Check if there are any corrupt records (for logging or reporting)
  val corruptRecordsDf = filteredDf.filter($"validation_flag" === "corrupt")
  val hasCorruptRecords = corruptRecordsDf.take(1).nonEmpty
  if (hasCorruptRecords) {
    println("There are corrupt records in the DataFrame.")
  } else {
    println("No corrupt records found.")
  }

  // Return the processed DataFrame as the final output
  filteredDf
}


========
import org.apache.spark.sql.{DataFrame, SparkSession}
import org.apache.spark.sql.functions._
import java.util.zip.{GZIPOutputStream, DeflaterOutputStream}
import java.io.{ByteArrayOutputStream, OutputStream}
import org.apache.spark.sql.types.BinaryType
import org.xerial.snappy.Snappy

// Define a function to get the appropriate compression output stream
def getCompressionStream(bos: ByteArrayOutputStream, method: String): OutputStream = method match {
  case "gzip" => new GZIPOutputStream(bos)
  case "zlib" => new DeflaterOutputStream(bos)
  case "snappy" => new ByteArrayOutputStream() // Snappy compression is handled differently
  case _ => throw new IllegalArgumentException(s"Unsupported compression method: $method")
}

// Define the UDF generator for compressing the column
def createCompressionUDF(method: String) = udf((input: String) => {
  if (input == null) {
    null
  } else {
    val bos = new ByteArrayOutputStream()
    
    // Handle Snappy differently since it does not use an OutputStream
    if (method == "snappy") {
      Snappy.compress(input.getBytes("UTF-8"))
    } else {
      val compressionStream = getCompressionStream(bos, method)
      compressionStream.write(input.getBytes("UTF-8"))
      compressionStream.close()
      bos.toByteArray
    }
  }
}, BinaryType)

// Define a function to compress a specified column with a chosen method
def compressColumn(df: DataFrame, columnName: String, method: String)(implicit spark: SparkSession): DataFrame = {
  val compressUDF = createCompressionUDF(method.toLowerCase)
  df.withColumn(s"${columnName}_compressed", compressUDF(col(columnName)))
}

// Usage example:
// Compress the "col1" column using GZIP, Snappy, or ZLIB
val method = "gzip"  // Choose "gzip", "snappy", or "zlib"
val compressedDF = compressColumn(df, "col1", method)

// Write the DataFrame to Snowflake
compressedDF.write
  .format("snowflake")
  .options(snowflakeOptions)  // Add your Snowflake connection options here
  .option("dbtable", "your_table_name")
  .save()

