import org.apache.spark.sql.SparkSession
import java.security.MessageDigest
import org.apache.spark.input.PortableDataStream

object CompactComputeHashes {
  def main(args: Array[String]): Unit = {
    val spark = SparkSession.builder.appName("Compact Compute Hashes").getOrCreate()
    val sc = spark.sparkContext
    val hdfsPath = "hdfs://path/to/your/files" // Update this path

    // Function to compute MD5 hash from a PortableDataStream
    def computeMD5(pds: PortableDataStream): String = {
      val md = MessageDigest.getInstance("MD5")
      md.digest(pds.toArray).map("%02x".format(_)).mkString
    }

    // Implicit class to convert PortableDataStream to Array[Byte]
    implicit class RichPortableDataStream(val pds: PortableDataStream) extends AnyVal {
      def toArray: Array[Byte] = Stream.continually(pds.open.read).takeWhile(-1 !=).map(_.toByte).toArray
    }

    // Read files, compute hashes, and collect
    val fileHashes = sc.binaryFiles(hdfsPath).mapValues(computeMD5).collect()

    // Generate master hash from concatenated file hashes
    val masterHash = MessageDigest.getInstance("MD5").digest(fileHashes.map(_._2).mkString.getBytes).map("%02x".format(_)).mkString

    println(s"Master Hash: $masterHash")

    spark.stop()
  }
}
