val rdd = spark.sparkContext.textFile(filePath)

// Accumulate lines in each partition and concatenate to a single string
val rawStr = rdd.mapPartitions { iter =>
  Iterator(iter.mkString("\n"))
}.collect().mkString("\n")



=======


import org.apache.hadoop.fs.{FileSystem, Path}
import scala.io.Source
import org.apache.spark.SparkContext

val fs = FileSystem.get(spark.sparkContext.hadoopConfiguration)
val path = new Path(filePath)

// Read the file using BufferedSource
val rawStr = {
  val stream = fs.open(path)
  try {
    Source.fromInputStream(stream).mkString
  } finally {
    stream.close()
  }
}
