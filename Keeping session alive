cd /path/to/framework_dq_materiality/
zip -r framework_dq_materiality.zip * 
cd C:\path\to\framework_dq_materiality
Compress-Archive -Path * -DestinationPath framework_dq_materiality.zip
pyspark --py-files /path/to/framework_dq_materiality.zip
# Test importing load_config from common
from common import load_config

# Load the configuration file
config = load_config("config/framework.conf")
print("Loaded Configuration:", config)

from common.utils import load_config
from common.log import get_logger

logger = get_logger("PySparkTestLogger")

config_file_path = "config/framework.conf"
config_data = load_config(config_file_path)
logger.info(f"Configuration Loaded: {config_data}")

Get-ChildItem *.zip


import sys
import os

# Append the path of the ZIP file to PYTHONPATH
zip_file_path = "framework_dq_materiality.zip"
sys.path.append(os.path.abspath(zip_file_path))

# Verify the path is added
print(sys.path)

===
# Initialize the logger
logger = get_logger("TestLogger")
logger.info("Logger initialized successfully.")

# Load the configuration
config_file_path = "config/framework.conf"
config = load_config(config_file_path)
logger.info(f"Loaded Config: {config}")
==========

def get_sample_dict():
    """
    Returns a sample dictionary for testing.
    """
    return {"key1": "value1", "key2": "value2", "key3": "value3"}
from .sample_data import get_sample_dict

========================

import logging

def get_logger(name="ApplicationLogger", level=logging.INFO):
    """
    Sets up and returns a reusable logger.

    Args:
        name (str): Name of the logger.
        level (int): Logging level (default is INFO).

    Returns:
        logging.Logger: Configured logger instance.
    """
    logger = logging.getLogger(name)
    if not logger.handlers:  # Prevent duplicate handlers
        logger.setLevel(level)
        
        # Console handler for logging
        console_handler = logging.StreamHandler()
        console_handler.setLevel(level)
        
        # Logging format
        formatter = logging.Formatter(
            '%(asctime)s - %(name)s - %(levelname)s - %(message)s'
        )
        console_handler.setFormatter(formatter)
        
        # Add the handler to the logger
        logger.addHandler(console_handler)
    
    return logger
from .log import get_logger
from common import get_logger

def main():
    # Initialize logger
    logger = get_logger("DriverLogger")
    
    logger.info("Starting the Spark job...")
    logger.warn("This is a warning log.")
    logger.error("This is an error log.")
    logger.info("Job completed successfully.")

if __name__ == "__main__":
    main()

