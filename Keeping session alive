import org.apache.hadoop.conf.Configuration
import org.apache.hadoop.fs.{FileSystem, Path}
import org.apache.spark.sql.SparkSession
import java.util.concurrent.TimeUnit

object SparkJobLockManagement {
  def main(args: Array[String]): Unit = {
    val spark = SparkSession.builder()
      .appName("Spark Job with Lock Management")
      .getOrCreate()

    val hdfsConf = new Configuration()
    val hdfs = FileSystem.get(hdfsConf)

    // Formatting today's date to include in the lock file name
    import java.time.LocalDate
    import java.time.format.DateTimeFormatter

    val formatter = DateTimeFormatter.ofPattern("yyyyMMdd")
    val todayFormatted = LocalDate.now().format(formatter)
    val lockFilePath = new Path(s"hdfs:///user/hive/warehouse/mydatabase/mytable/_ingestion_$todayFormatted.lock")

    // Function to check for the existence of the lock file
    def lockFileExists(): Boolean = hdfs.exists(lockFilePath)

    // Function to wait for the lock file to be removed
    def waitForLockToClear(): Boolean = {
      while (lockFileExists()) {
        println(s"Lock file $lockFilePath exists. Waiting for ingestion job to complete...")
        TimeUnit.MINUTES.sleep(5) // Check every 5 minutes
      }
      true // Return true once the lock file no longer exists
    }

    // Wait for any existing lock to clear before proceeding
    if (waitForLockToClear()) {
      // Create a lock file to signify the start of this job's processing
      def createLockFile(): Unit = {
        hdfs.createNewFile(lockFilePath)
        println(s"Lock file $lockFilePath created.")
      }

      // Logic for your job's processing
      // -----------------------------------------
      // Insert your data processing logic here
      // -----------------------------------------

      // Function to remove the lock file after processing is complete
      def removeLockFile(): Unit = {
        if (hdfs.exists(lockFilePath)) {
          hdfs.delete(lockFilePath, false)
          println(s"Lock file $lockFilePath removed.")
        }
      }

      // Remove the lock file upon successful completion of the job
      removeLockFile()
    }

    // Close the Spark session at the end of your application
    spark.stop()
  }
}

====// Extract the table's location (HDFS path) from the DataFrame
val locationRow = describeTableDF
  .filter($"col_name" === "Location")
  .select("data_type")
  .first()
