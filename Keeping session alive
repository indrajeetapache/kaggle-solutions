dataProcessingConfig {
  sparkReadOptions {
    header = "true"
    inferSchema = "true"
    delimiter = ","
    # Add more read options as needed
  }
expressions = {
    controlColumn1 = "split(value, ',')[0]"
    controlColumn2 = "split(value, ',')[1]"
    # Add more expressions as needed
  }
}
===
val expressionsMap: Map[String, String] = dataProcessingConf.getConfig("expressions").entrySet().asScala.map { entry =>
  entry.getKey -> entry.getValue.unwrapped().toString
}.toMap

// Assuming `df` is the original DataFrame which has the 'value' column
val df: DataFrame = // Load your DataFrame here

val transformedDF: DataFrame = expressionsMap.foldLeft(df) { (tempDF, expression) =>
  tempDF.withColumn(expression._1, expr(expression._2))
}
===

import org.apache.spark.sql.{SparkSession, DataFrame}
import com.typesafe.config.ConfigFactory

import com.typesafe.config.ConfigFactory
import scala.collection.JavaConverters._

val conf = ConfigFactory.load()

// Get the hiveUserDefined Config object
val hiveUserDefinedConfig = conf.getConfig("sparkReadOptions.hiveUserDefined")

// Extract each entry as a key-value pair and convert it to a SQL expression
val columnExpressions: Seq[String] = hiveUserDefinedConfig.entrySet().asScala.toSeq.map { entry =>
  val key = entry.getKey
  val value = hiveUserDefinedConfig.getString(key)
  s"$key = $value"
}

// The rest of your code to apply these expressions to your DataFrame


// Assuming df is your original DataFrame which has the 'value' column
val df: DataFrame = // ... load your DataFrame

// Register the DataFrame as a temporary view
df.createOrReplaceTempView("temp_view")

// Construct the SQL query with the expressions
val sqlQuery = s"SELECT ${columnExpressions.mkString(", ")} FROM temp_view"

// Execute the SQL query
val transformedDF = spark.sql(sqlQuery)

// Show the result
transformedDF.show()

// Stop the Spark session
spark.stop()


df.withColumn("extractedValue", expr("regexp_extract(col1, '162648\\|(\\d+)', 1)")).show(false)
df.withColumn("extractedValue", expr("regexp_extract(col1, '162648\\|\\s*(.*?)\\s*\\|', 1)")).show(false)
df.withColumn("thirdValue", split(col("col1"), "\\|")(2)).show(false)
====

import org.apache.spark.sql.DataFrame
import org.apache.spark.sql.functions.{col, lit}
import org.apache.spark.sql.types.StructType

// Function to align the schema of df2 to match df
def alignSchema(originalDf: DataFrame, targetSchema: StructType): DataFrame = {
  val adjustedColumns = targetSchema.fields.map { field =>
    // Check if originalDf contains the column
    if (originalDf.columns.contains(field.name)) {
      // Column exists, cast it to the target type
      col(field.name).cast(field.dataType).as(field.name)
    } else {
      // Column does not exist, add it as null with the target type
      lit(null).cast(field.dataType).as(field.name)
    }
  }

  originalDf.select(adjustedColumns: _*)
}

// Assuming df is the DataFrame with the schema directly from Snowflake
// and df2 is the DataFrame with actual data where nulls might have changed the inferred types
val df2Aligned = alignSchema(df2, df.schema)

// Now df2Aligned's schema matches df, including correct handling of nulls and data types
// You can proceed to write df2Aligned to Snowflake



