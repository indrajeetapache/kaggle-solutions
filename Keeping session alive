import org.apache.spark.sql.{DataFrame, SparkSession}
import org.apache.spark.sql.functions._
import org.apache.spark.sql.expressions.Window

val spark = SparkSession.builder()
  .appName("example")
  .master("local[*]")  // Adjust based on your setup
  .getOrCreate()

import spark.implicits._

// Sample DataFrame
val df = Seq(
  ("A"),
  ("A"),
  ("B"),
  ("B"),
  ("B"),
  ("C"),
  ("C"),
  ("D"),
  ("D")
).toDF("key")

// Function to perform recursive aggregation
def recursiveAggregation(df: DataFrame, callCount: Int = 1): DataFrame = {
  // Log the call count
  println(s"Recursive call count: $callCount")

  // Define a window specification with an orderBy clause
  val windowSpec = Window.orderBy(lit(1))  // Here, lit(1) is a dummy value to order by

  // Generate a row number for each row
  val dfWithId = df.withColumn("value", row_number().over(windowSpec))

  // Get the maximum value in the value column, which indicates the total count
  val maxId = dfWithId.agg(max("value")).head.getLong(0)

  // If maxId is 1, it means there's only one row, so return the DataFrame
  if (maxId == 1) {
    return dfWithId
  }

  // Calculate the range dynamically
  val range = maxId match {
    case count if count < 50 => 2
    case count => (count / 10).toInt
  }

  // Add a partition ID to each row based on the calculated range
  val dfWithPartitionId = dfWithId.withColumn("partition_id", floor(col("value") / range))

  // Group by the partition ID and key, then aggregate
  val aggregatedDF = dfWithPartitionId.groupBy("partition_id", "key")
    .agg(md5(concat_ws("", collect_list(dfWithPartitionId("value").cast("string")))).alias("hash"))

  // Call the function recursively with the new DataFrame, incrementing the call count
  recursiveAggregation(aggregatedDF.drop("partition_id", "value"), callCount + 1)
}

// Call the recursive aggregation function
val resultDF = recursiveAggregation(df)
resultDF.show()
