from pyspark.sql import SparkSession

def load_config(config_path):
    # Load configuration file
    framework_config = Load_config(config_path)
    materiality_core_dict = {
        key: value for key, value in 
        framework_config['Dataset_Input_Query'].items()
    }
    return materiality_core_dict

def main():
    # Initialize Spark Session
    spark = SparkSession.builder \
        .appName("MaterialityAnalysis") \
        .getOrCreate()

    # Get config path from command line arguments
    config_path = spark.conf.get("spark.config.path")
    
    try:
        # Load configuration
        config_dict = load_config(config_path)
        
        # Your processing logic here
        print("Framework Configuration Loaded Successfully")
        
    finally:
        spark.stop()

if __name__ == "__main__":
    main()


spark-submit \
--master yarn \
--deploy-mode cluster \
--conf spark.config.path=/path/to/bcd_serv_crd_arrg_dim_dq_collibraMaterllity.conf \
--files /path/to/bcd_serv_crd_arrg_dim_dq_collibraMaterllity.conf \
--driver-memory 4g \
--executor-memory 8g \
--executor-cores 2 \
--num-executors 4 \
--queue your_queue_name \
--name "Materiality_Analysis_Job" \
materiality_job.py
