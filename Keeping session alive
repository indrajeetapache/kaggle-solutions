val rdd = spark.sparkContext.textFile(filePath)

// Accumulate lines in each partition and concatenate to a single string
val rawStr = rdd.mapPartitions { iter =>
  Iterator(iter.mkString("\n"))
}.collect().mkString("\n")



=======


import org.apache.hadoop.fs.{FileSystem, Path}
import scala.io.Source
import org.apache.spark.SparkContext

val fs = FileSystem.get(spark.sparkContext.hadoopConfiguration)
val path = new Path(filePath)

// Read the file using BufferedSource
val rawStr = {
  val stream = fs.open(path)
  try {
    Source.fromInputStream(stream).mkString
  } finally {
    stream.close()
  }
}


====

import cats.effect.{IO, Resource}
import fs2.Stream
import fs2.io.readInputStream
import org.apache.hadoop.conf.Configuration
import org.apache.hadoop.fs.{FileSystem, Path}

// Function to read a file from HDFS line by line and return as a single String
def readHDFSFileLineByLine(hdfsPath: String): IO[String] = {
  val configuration = new Configuration()
  val fs = FileSystem.get(configuration)

  def inputStreamResource: Resource[IO, java.io.InputStream] = Resource.make {
    IO(fs.open(new Path(hdfsPath)))  // Acquire InputStream from HDFS
  } { inputStream =>
    IO(inputStream.close()).handleErrorWith(_ => IO.unit)  // Ensure stream is closed
  }

  inputStreamResource.use { inputStream =>
    readInputStream(IO.pure(inputStream), chunkSize = 8192, closeAfterUse = false)
      .through(fs2.text.utf8.decode)   // Decode byte stream to text
      .through(fs2.text.lines)         // Split the decoded text into lines
      .intersperse("\n")               // Add line separators back (preserving line breaks)
      .compile
      .string                          // Concatenate all lines into a single string
  }
}


<dependencies>
    <!-- FS2 Core and IO modules -->
    <dependency>
        <groupId>co.fs2</groupId>
        <artifactId>fs2-core_2.13</artifactId>
        <version>3.2.7</version> <!-- Use the latest stable version -->
    </dependency>
    <dependency>
        <groupId>co.fs2</groupId>
        <artifactId>fs2-io_2.13</artifactId>
        <version>3.2.7</version>
    </dependency>

    <!-- Cats Effect for IO and Resource Management -->
    <dependency>
        <groupId>org.typelevel</groupId>
        <artifactId>cats-effect_2.13</artifactId>
        <version>3.3.14</version> <!-- Use the latest stable version -->
    </dependency>
</dependencies>

=================

import cats.effect.{IO, Resource}
import fs2.Stream
import fs2.io.readInputStream
import org.apache.hadoop.conf.Configuration
import org.apache.hadoop.fs.{FileSystem, Path}

// Function to read a file from HDFS line by line and return as a single String
def readHDFSFileLineByLine(hdfsPath: String): IO[String] = {
  val configuration = new Configuration()
  val fs = FileSystem.get(configuration)

  // Create an InputStream resource for managing the HDFS InputStream lifecycle
  def inputStreamResource: Resource[IO, java.io.InputStream] = Resource.make {
    IO(fs.open(new Path(hdfsPath)))  // Acquire InputStream from HDFS
  } { inputStream =>
    IO(inputStream.close()).handleErrorWith(_ => IO.unit)  // Ensure the stream is closed on release
  }

  inputStreamResource.use { inputStream =>
    readInputStream(IO.pure(inputStream), chunkSize = 8192) // `closeAfterUse` is not required in fs2 2.x
      .through(fs2.text.utf8Decode)  // Decode byte stream to text
      .through(fs2.text.lines)       // Split the decoded text into lines
      .intersperse("\n")             // Add line separators back to preserve line breaks
      .compile
      .string                        // Concatenate all lines into a single string
  }
}

========

import cats.effect.{ContextShift, IO, Resource}
import fs2.Stream
import fs2.io.readInputStream
import org.apache.hadoop.conf.Configuration
import org.apache.hadoop.fs.{FileSystem, Path}
import scala.concurrent.ExecutionContext

// Define your ContextShift and Blocker
implicit val cs: ContextShift[IO] = IO.contextShift(ExecutionContext.global)

def readHDFSFileLineByLine(hdfsPath: String): IO[String] = {
  val configuration = new Configuration()
  val fs = FileSystem.get(configuration)

  // Resource for HDFS InputStream with logging
  def inputStreamResource: Resource[IO, java.io.InputStream] = Resource.make {
    IO {
      println(s"Opening HDFS file at path: $hdfsPath")
      fs.open(new Path(hdfsPath))
    }
  } { inputStream =>
    IO {
      println(s"Closing HDFS file at path: $hdfsPath")
      inputStream.close()
    }.handleErrorWith { ex =>
      IO(println(s"Error closing HDFS file at path: $hdfsPath: $ex"))
    }
  }

  inputStreamResource.use { inputStream =>
    readInputStream(IO.pure(inputStream), chunkSize = 8192) // Removed `blocker`
      .through(fs2.text.utf8Decode)
      .through(fs2.text.lines)
      .intersperse("\n")
      .compile
      .string
      .flatTap(content => IO(println(s"Finished reading HDFS file at path: $hdfsPath, content size: ${content.length}")))
  }
}
==============


import cats.effect.{ContextShift, IO, Resource, Sync}
import fs2.Stream
import fs2.io.readInputStream
import org.apache.hadoop.conf.Configuration
import org.apache.hadoop.fs.{FileSystem, Path, FSDataInputStream}
import scala.concurrent.ExecutionContext

// Define your ContextShift and Sync instances
implicit val cs: ContextShift[IO] = IO.contextShift(ExecutionContext.global)
implicit val syncIO: Sync[IO] = IO.ioConcurrentEffect

def readHDFSFileLineByLine(hdfsPath: String): IO[String] = {
  val configuration = new Configuration()
  val fs = FileSystem.get(configuration)

  // Resource for HDFS InputStream with logging
  def inputStreamResource: Resource[IO, java.io.InputStream] = Resource.make {
    IO {
      println(s"Opening HDFS file at path: $hdfsPath")
      fs.open(new Path(hdfsPath))  // fs.open returns FSDataInputStream
    }.widen[InputStream]  // Widen to InputStream to match expected type
  } { inputStream =>
    IO {
      println(s"Closing HDFS file at path: $hdfsPath")
      inputStream.close()
    }.handleErrorWith { ex =>
      IO(println(s"Error closing HDFS file at path: $hdfsPath: $ex"))
    }
  }

  inputStreamResource.use { inputStream =>
    readInputStream(IO.pure(inputStream), chunkSize = 8192)
      .through(fs2.text.utf8Decode)
      .through(fs2.text.lines)
      .intersperse("\n")
      .compile
      .string
      .flatTap(content => IO(println(s"Finished reading HDFS file at path: $hdfsPath, content size: ${content.length}")))
  }
}
=============


import cats.effect.{Blocker, ContextShift, IO, Resource, Sync}
import fs2.Stream
import fs2.io.readInputStream
import org.apache.hadoop.conf.Configuration
import org.apache.hadoop.fs.{FileSystem, Path, FSDataInputStream}
import scala.concurrent.ExecutionContext

// Define your ContextShift and Blocker
implicit val cs: ContextShift[IO] = IO.contextShift(ExecutionContext.global)
val blocker: Blocker = Blocker.liftExecutionContext(ExecutionContext.global)

def readHDFSFileLineByLine(hdfsPath: String): IO[String] = {
  val configuration = new Configuration()
  val fs = FileSystem.get(configuration)

  // Resource for HDFS InputStream with logging
  def inputStreamResource: Resource[IO, java.io.InputStream] = Resource.make {
    IO {
      println(s"Opening HDFS file at path: $hdfsPath")
      fs.open(new Path(hdfsPath))
    }
  } { inputStream =>
    IO {
      println(s"Closing HDFS file at path: $hdfsPath")
      inputStream.close()
    }.handleErrorWith { ex =>
      IO(println(s"Error closing HDFS file at path: $hdfsPath: $ex"))
    }
  }

  inputStreamResource.use { inputStream =>
    readInputStream(
      IO.pure(inputStream),
      chunkSize = 8192,
      blocker = blocker,       // Provide the blocker
      closeAfterUse = true     // Set to true or false as needed
    )
    .through(fs2.text.utf8Decode)
    .through(fs2.text.lines)
    .intersperse("\n")
    .compile
    .string
    .flatTap(content => IO(println(s"Finished reading HDFS file at path: $hdfsPath, content size: ${content.length}")))
  }
}

import cats.effect.{Blocker, ContextShift, IO, Sync}
import scala.concurrent.ExecutionContext

implicit val cs: ContextShift[IO] = IO.contextShift(ExecutionContext.global)
implicit val syncIO: Sync[IO] = IO.ioConcurrentEffect
implicit val blocker: Blocker = Blocker.liftExecutionContext(ExecutionContext.global)

val filePath: String = "hdfs://path/to/your/file"
val rawStr: String = util.readHDFSFileLineByLine(filePath).unsafeRunSync()

logger.info(s"Raw string content: $rawStr")

readInputStream: This fs2 function reads data from an InputStream and converts it into a Stream[IO, Byte] (a stream of bytes).
IO.pure(inputStream): Wraps the InputStream in an IO action to make it compatible with fs2.
chunkSize = 8192: Specifies the size (in bytes) of each chunk to be read from the InputStream. Reading in chunks helps to control memory usage by not loading the entire file into memory at once.
blocker = blocker: Tells readInputStream to use the Blocker, ensuring that reading from the InputStream (a blocking operation) happens on a separate thread pool.
closeAfterUse = true: Ensures the InputStream will be closed automatically after reading, adding an extra layer of safety in resource management.
through(fs2.text.utf8Decode): This converts the byte stream into a character stream with UTF-8 encoding. It transforms the Stream[IO, Byte] into a Stream[IO, String], where each element is a UTF-8 decoded string from the bytes.

through(fs2.text.lines): This breaks the decoded text into lines. It further transforms the Stream[IO, String] into a Stream[IO, String] where each element represents a line of text in the file.
.intersperse("\n"): This inserts newline characters (\n) between each line in the stream, preserving the original line structure of the file. Without this, concatenating the lines might result in a continuous string without line breaks.
.compile.string: This operation compiles the entire Stream[IO, String] (each element representing a line) into a single String, effectively concatenating all lines in the file into one continuous string with line breaks.

===========

import org.apache.hadoop.conf.Configuration
import org.apache.hadoop.fs.{FileSystem, Path}
import scala.io.Source

def readLargeFileAsSingleString(hdfsPath: String): String = {
  val conf = new Configuration()
  val fs = FileSystem.get(conf)
  val path = new Path(hdfsPath)
  val inputStream = fs.open(path)
  
  try {
    val lines = Source.fromInputStream(inputStream).getLines()
    val content = new StringBuilder
    
    // Append each line to the StringBuilder
    lines.foreach { line =>
      content.append(line).append("\n")
    }
    
    // Return the entire content as a single string
    content.toString
  } finally {
    inputStream.close()
  }
}

// Usage
val filePath = "hdfs://path/to/your/3gb_file"
val rawStr: String = readLargeFileAsSingleString(filePath)
println(s"File content size: ${rawStr.length}")


spark-submit \
  --conf spark.sql.crossJoin.enabled=true \
  --conf spark.dynamicAllocation.minExecutors=12 \
  --conf spark.dynamicAllocation.initialExecutors=15 \
  --conf spark.dynamicAllocation.maxExecutors=20 \
  --conf spark.sql.parquet.compression.codec=snappy \
  --deploy-mode cluster \
  --driver-memory 8g \
  --executor-memory 12g \
  --conf spark.yarn.am.memoryOverhead=6g \
  --conf spark.yarn.driver.memoryOverhead=3g \
  --conf spark.yarn.executor.memoryOverhead=2g \
  --queue root.Grmas_3170_Yarn \
  path/to/your/application.jar

=======

spark-submit \
  --master yarn \
  --deploy-mode cluster \
  --name campgenrolletxt \
  --driver-memory 20g \
  --executor-memory 16g \
  --conf spark.yarn.driver.memoryOverhead=6g \
  --conf spark.yarn.executor.memoryOverhead=4g \
  --conf spark.dynamicAllocation.enabled=true \
  --conf spark.dynamicAllocation.maxExecutors=20 \
  --conf spark.hadoop.mapreduce.input.fileinputformat.split.minsize=536870912 \
  --conf spark.hadoop.mapreduce.input.fileinputformat.split.maxsize=1073741824 \
  --conf "spark.executor.extraJavaOptions=-Xmx16g" \
  --conf spark.sql.files.maxPartitionBytes=536870912 \
  --conf spark.sql.files.openCostInBytes=134217728 \
  <path-to-your-application-jar> <your-application-arguments>


val rdd = spark.sparkContext.textFile(filePath)
val entireFileAsString = rdd.reduce(_ + "\n" + _)
val entireFileAsString = coalescedRdd.aggregate("")(
  (acc, line) => if (acc.isEmpty) line else acc + "\n" + line,  // Within-partition aggregation
  (acc1, acc2) => acc1 + "\n" + acc2                             // Across-partition aggregation
)
============

import org.apache.spark.sql.{DataFrame, SparkSession, Row}
import org.apache.spark.sql.types._
import org.apache.spark.sql.functions._
import java.nio.file.Paths

def processCsvWithMultiDelimiter(
    spark: SparkSession,
    filePath: String,
    schema: StructType,                          // Schema provided by the user
    delimiters: Array[Char],                     // Mandatory: Array of delimiters
    replacementChar: Option[Char] = None         // Optional: Replacement character
): DataFrame = {

  import spark.implicits._

  // Calculate expectedFieldCount from the schema
  val expectedFieldCount = schema.fields.length

  // Get the source file name
  val sourceFileName = Paths.get(filePath).getFileName.toString

  // Read the file as a DataFrame with a single column "line"
  val df = spark.read
    .text(filePath)
    .withColumnRenamed("value", "line")

  // Step 1: Replace delimiters using either replacementChar or the first delimiter in delimiters
  val effectiveReplacementChar = replacementChar.getOrElse(delimiters.head)
  val delimiterPattern = delimiters.map(Regex.quote).mkString("|")  // e.g., "\|,|;"
  
  val processedDf = df.withColumn("line", regexp_replace($"line", delimiterPattern, effectiveReplacementChar.toString))

  // Step 2: Create a flag column to identify lines with unexpected field counts after replacement
  val flaggedDf = processedDf.withColumn(
    "flag",
    when(size(split($"line", "\\" + effectiveReplacementChar)) === expectedFieldCount, 0).otherwise(1)
  )

  // Step 3: Process partitions to join lines where "flag" is 1, indicating incomplete fields
  val joinedRdd = flaggedDf
    .select("line", "flag")
    .as[(String, Int)]
    .rdd
    .mapPartitions(partition => {
      val buffer = scala.collection.mutable.ListBuffer[Row]()
      var currentLine = new StringBuilder

      partition.foreach {
        case (line, flag) =>
          if (flag == 0) {
            // If flag is 0, add the current accumulated line to buffer, then add the valid line
            if (currentLine.nonEmpty) {
              buffer += Row(currentLine.toString, null)
              currentLine.clear()
            }
            buffer += Row(line, null)
          } else {
            // If flag is 1, accumulate the line until we reach a valid field count
            if (currentLine.nonEmpty) currentLine.append(effectiveReplacementChar)
            currentLine.append(line)
          }
      }

      // Add any remaining accumulated line as corrupt if it still doesn’t meet the field count
      if (currentLine.nonEmpty) {
        val fields = currentLine.toString.split("\\|", -1)
        if (fields.length == expectedFieldCount) {
          buffer += Row(currentLine.toString, null)
        } else {
          buffer += Row(null, currentLine.toString)  // Mark as corrupt record
        }
      }
      buffer.iterator
    })

  // Step 4: Convert joined RDD to DataFrame with corrupt record handling
  val rowRdd = joinedRdd.map {
    case Row(line: String, null) => Row.fromSeq(line.split("\\" + effectiveReplacementChar, -1).toSeq ++ Seq(null, sourceFileName, filePath))
    case Row(null, corruptLine: String) => Row.fromSeq(Seq.fill(expectedFieldCount)(null) ++ Seq(corruptLine, sourceFileName, filePath))
  }

  // Step 5: Define schema with additional columns
  val extendedSchema = StructType(schema.fields ++ Seq(
    StructField("corrupt_record", StringType, true),
    StructField("source_file_name", StringType, true),
    StructField("source_file_path", StringType, true)
  ))

  // Create the final cleaned DataFrame
  spark.createDataFrame(rowRdd, extendedSchema)
}
// Path to the input file
val filePath = "path/to/your/file.csv"

// Define the delimiters and replacement character
val delimiters = Array('|', '|')  // Delimiter is "||"
val replacementChar = Some('|')    // Replacement character is "|"

// Call the function with these parameters
val resultDf = processCsvWithMultiDelimiter(spark, filePath, schema, delimiters, replacementChar)

// Show the final cleaned DataFrame
resultDf.show(truncate = false)
