import org.apache.spark.scheduler._
import org.apache.spark.SparkContext

// Define a custom SparkListener
class CustomSparkListener extends SparkListener {

  override def onApplicationEnd(applicationEnd: SparkListenerApplicationEnd): Unit = {
    println(s"Application has ended at time ${applicationEnd.time}")
  }

  override def onJobStart(jobStart: SparkListenerJobStart): Unit = {
    println(s"Job started with job id ${jobStart.jobId} at time ${jobStart.time}")
  }

  override def onJobEnd(jobEnd: SparkListenerJobEnd): Unit = {
    println(s"Job ended with job id ${jobEnd.jobId} at time ${jobEnd.time}")
  }

  override def onStageCompleted(stageCompleted: SparkListenerStageCompleted): Unit = {
    println(s"Stage completed with stage id ${stageCompleted.stageInfo.stageId}")
  }

  override def onStageSubmitted(stageSubmitted: SparkListenerStageSubmitted): Unit = {
    println(s"Stage submitted with stage id ${stageSubmitted.stageInfo.stageId}")
  }

  override def onTaskStart(taskStart: SparkListenerTaskStart): Unit = {
    println(s"Task started with task id ${taskStart.taskInfo.taskId} on stage ${taskStart.stageId}")
  }

  override def onTaskGettingResult(taskGettingResult: SparkListenerTaskGettingResult): Unit = {
    println(s"Task getting result with task id ${taskGettingResult.taskInfo.taskId}")
  }

  override def onTaskEnd(taskEnd: SparkListenerTaskEnd): Unit = {
    println(s"Task ended with task id ${taskEnd.taskInfo.taskId} with reason ${taskEnd.reason}")
  }
}

// Now you can add your custom listener to the SparkContext
val sc: SparkContext = new SparkContext(conf) // assuming conf is already defined as SparkConf

// Add the custom listener
sc.addSparkListener(new CustomSparkListener)

// Your Spark application logic here

// Stop the SparkContext at the end of your application
sc.stop()
