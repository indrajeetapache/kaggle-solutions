dataProcessingConfig {
  sparkReadOptions {
    header = "true"
    inferSchema = "true"
    delimiter = ","
    # Add more read options as needed
  }
expressions = {
    controlColumn1 = "split(value, ',')[0]"
    controlColumn2 = "split(value, ',')[1]"
    # Add more expressions as needed
  }
}
===
val expressionsMap: Map[String, String] = dataProcessingConf.getConfig("expressions").entrySet().asScala.map { entry =>
  entry.getKey -> entry.getValue.unwrapped().toString
}.toMap

// Assuming `df` is the original DataFrame which has the 'value' column
val df: DataFrame = // Load your DataFrame here

val transformedDF: DataFrame = expressionsMap.foldLeft(df) { (tempDF, expression) =>
  tempDF.withColumn(expression._1, expr(expression._2))
}
===

import org.apache.spark.sql.{SparkSession, DataFrame}
import com.typesafe.config.ConfigFactory

// Initialize SparkSession
val spark: SparkSession = SparkSession.builder()
  .appName("DataFrame Transformation with Configurations")
  .getOrCreate()

// Load the .conf file and read the hiveUserDefinedColumns configuration
val conf = ConfigFactory.load()
val hiveUserDefinedColumnsString: String = conf.getString("dataProcessingConfig.expressions.hiveUserDefinedColumns")

// Parse the string into an array of column expressions
val columnExpressions: Array[String] = hiveUserDefinedColumnsString.split(",")
  .map(_.trim)
  .filter(_.nonEmpty)

// Assuming df is your original DataFrame which has the 'value' column
val df: DataFrame = // ... load your DataFrame

// Register the DataFrame as a temporary view
df.createOrReplaceTempView("temp_view")

// Construct the SQL query with the expressions
val sqlQuery = s"SELECT ${columnExpressions.mkString(", ")} FROM temp_view"

// Execute the SQL query
val transformedDF = spark.sql(sqlQuery)

// Show the result
transformedDF.show()

// Stop the Spark session
spark.stop()
