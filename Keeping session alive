e// First, let's add more robust error handling options
val df: DataFrame = spark.read
  .schema(schema)
  .option("header", "false")
  .option("multiline", "true")
  .option("encoding", "UTF-8")
  .option("delimiter", "\u0001")
  .option("mode", "PERMISSIVE")  // Try PERMISSIVE instead of DROPMALFORMED
  .option("columnNameOfCorruptRecord", "_corrupt_record")  // Track corrupt records
  .option("nullValue", "")  // Handle empty fields
  .option("maxCharsPerColumn", 100000)  // Increase if you have large fields
  .csv("/data/gcgeaapnatldu/raw/169912/sow_ih_payer_test/2025/01/06/batch5/sow_ih_payer_20240822.dat")

// Now let's check for corrupt records
df.filter($"_corrupt_record".isNotNull).show(false)

// Get count of good vs bad records
println(s"Total records: ${df.count()}")
println(s"Corrupt records: ${df.filter($"_corrupt_record".isNotNull).count()}")
